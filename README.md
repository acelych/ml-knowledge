# 机器学习基础知识

## 1. 数值计算

数值计算的核心目标是：对于那些难以或无法求得精确解的数学问题，设计出高效的算法来寻找其**近似解**。

### 1.1 求解非线性方程 f(x) = 0

目标是找到方程的根 `x`。

#### 直接迭代法/不动点迭代法 (Fixed-Point Iteration)

任何非线性方程 `f(x) = 0`，我们都可以通过代数变形，将其转换为等价的**不动点方程**：
$$x = g(x)$$
一个满足 `x* = g(x*)` 的点 `x*`，就称为函数 `g(x)` 的一个**不动点 (Fixed Point)**。显然，`g(x)` 的不动点就是原方程 `f(x)=0` 的解。

直接迭代法，就是基于这个形式构造一个迭代序列：
1.  任选一个初始猜测值 `x₀`。
2.  反复使用迭代公式进行计算：
    $$x_{k+1} = g(x_k)$$
3.  如果这个序列收敛，其极限 `x*` 就是我们要求的解。

牛顿法是我们遇到的第一个特例。对于方程 `f(x)=0`，牛顿法构造的迭代函数是：
$$g(x) = x - \frac{f(x)}{f'(x)}$$
它是一种形式非常特殊且高效的不动点迭代。

#### 收敛定理（一阶导数判别法）

这个几何直观可以被严格地表述为以下定理：

> 如果不动点 `x*` 存在于某个区间 `I` 内，并且函数 `g(x)` 在该区间上满足以下两个条件：
> 1.  对任意 `x ∈ I`，都有 `g(x) ∈ I`。（迭代过程不会“跑出”这个区间）
> 2.  存在一个常数 `L < 1`，使得对所有 `x ∈ I`，都满足 $|g'(x)| \le L$。（`g(x)` 是一个**压缩映射 (Contraction Mapping)**）
>
> 那么，对于任意初始点 `x₀ ∈ I`，不动点迭代 `xₖ₊₁ = g(xₖ)` **必然收敛**到唯一的不动点 `x*`。

**应试核心结论**:
> **迭代收敛的一个充分条件是，在不动点 `x*` 附近，`|g'(x*)| < 1`。**
> - `|g'(x*)|` 的值越小，收敛速度越快。
> - 如果 `|g'(x*)| > 1`，迭代几乎必然发散。
> - 如果 `|g'(x*)| = 1`，收敛性不确定，可能收敛也可能发散。

**压缩映射** (Contraction Mapping)
- **思想**: 如果一个函数 ϕ(x) 能把一个区间内的任意两点的距离“压缩”得更近，那么它就是这个区间上的一个压缩映射。
- **直观理解**: 如果每次迭代都能让你离真解 x* 更近一点，那么这个过程最终必然会收敛。

$|x_{k+1}-x^*|=|g(x_k)-g(x^*)|<|x_k-x^*|$

要满足这个“压缩”条件，就需要函数 ϕ(x) 的变化率足够小。

#### 牛顿迭代法 (Newton's Method)
- **思想**: 在当前点 `xₖ` 做一条切线，用切线与x轴的交点作为下一次的近似解 `xₖ₊₁`。
- **公式**:
  $$x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}$$
- **特点**:
    - **优点**: 在根附近收敛速度极快（二次收敛）。
    - **缺点**: 需要计算导数 `f'(x)`；初始猜测值 `x₀` 需要离真解足够近，否则可能不收敛。

牛顿法的迭代函数 $g(x) = x - \frac{f(x)}{f'(x)}$。
我们对它求导，可以得到：
$$g'(x) = \frac{f(x)f''(x)}{[f'(x)]^2}$$
在不动点（也就是真解）`x*` 处，`f(x*) = 0`，所以：
$$g'(x^*) = 0$$
因为 `0 < 1`，这完美地满足了收敛条件！`g'(x*)` 等于0是它能够实现二次快速收敛的根本原因。这也解释了为什么牛顿法是如此高效和著名。

#### 二分法 (Bisection Method)
- **思想**: 保证根在一个区间 `[a, b]` 内 (即 `f(a)` 和 `f(b)` 异号)，然后不断将区间对半分割，每次都保留包含根的那一半区间。
- **步骤**:
    1. 找到 `a, b` 使得 `f(a)·f(b) < 0`。
    2. 计算中点 `c = (a+b)/2`。
    3. 如果 `f(a)·f(c) < 0`，则根在 `[a, c]` 内，令 `b=c`。否则根在 `[c, b]` 内，令 `a=c`。
    4. 重复2-3步，直到区间足够小。
- **特点**:
    - **优点**: 算法简单，只要初始区间有根，就一定能收敛。
    - **缺点**: 收敛速度慢（线性收敛）。

### 1.2 求解线性方程组 Ax = b (迭代法)

适用于大型稀疏矩阵，直接求解（如求逆）计算量过大。

#### 雅可比迭代法 (Jacobi Method)

雅可比迭代法是一种用于求解线性方程组 $Ax=b$ 的基础算法。其核心思想是：从一个初始猜测解出发，通过反复迭代，不断用上一轮的解来更新每一个变量，直到解的精度满足要求为止。它特别适用于系数矩阵 `A` 是**大型稀疏矩阵**的场景。

假设我们有如下 n 元线性方程组：
$$
\begin{cases}
a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n = b_1 \\
a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n = b_2 \\
\vdots \\
a_{n1}x_1 + a_{n2}x_2 + \dots + a_{nn}x_n = b_n
\end{cases}
$$

Step 1: 构造迭代公式
将第 `i` 个方程变形，用其他变量来表示 $x_i$：
$$x_i = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j \right)$$

将其写成迭代形式，用第 `k` 轮的解 $(x_1^{(k)}, x_2^{(k)}, \dots)$ 计算第 `k+1` 轮的解：
$$x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)$$

Step 2: 执行迭代
1.  选择一个初始解向量 $x^{(0)}$，通常是零向量。
2.  将 $x^{(0)}$ 代入迭代公式，计算出 $x^{(1)}$。
3.  将 $x^{(1)}$ 代入迭代公式，计算出 $x^{(2)}$。
4.  ...
5.  重复此过程，直到相邻两次迭代结果的差值足够小（例如 $|x^{(k+1)} - x^{(k)}| < \epsilon$），或者达到预设的最大迭代次数。

**收敛性判断**

雅可比法**不保证**对所有矩阵都收敛。

* **充分条件（易于判断）**: 系数矩阵 `A` 是**严格对角占优 (Strictly Diagonally Dominant)**。
    * **定义**: 矩阵每一行，对角元素的绝对值 **大于** 该行所有非对角元素的绝对值之和。
    * **公式**: 对于所有 $i$, 都有 $|a_{ii}| > \sum_{j \neq i} |a_{ij}|$
    * 只要满足此条件，迭代过程**必定收敛**。

* **充要条件（理论核心）**: 迭代矩阵 $T_J = -D^{-1}(L+U)$ 的**谱半径** $\rho(T_J) < 1$。
    * 其中 `D` 是 `A` 的对角部分，`L` 是 `A` 的严格下三角部分，`U` 是 `A` 的严格上三角部分。

**示例（考试原题）**

方程组:
$$
\begin{cases}
4x + y = 6 \\
x + 3y = 3
\end{cases}
$$

迭代公式:
$$
x_{k+1} = \frac{6 - y_k}{4}
$$
$$
y_{k+1} = \frac{3 - x_k}{3}
$$

第一次迭代:
* 初始解: $(x_0, y_0) = (0, 0)$
* 计算:
    * $x_1 = (6 - 0) / 4 = 1.5$
    * $y_1 = (3 - 0) / 3 = 1$
* 结果: $(x_1, y_1) = (1.5, 1)$

**优缺点**

* **优点**:
    * 算法逻辑简单，易于编程实现。
    * 每次迭代中，各个分量的计算是独立的，非常适合**并行计算**。
* **缺点**:
    * 收敛性不能保证。
    * 即使收敛，收敛速度通常也比高斯-赛德尔（Gauss-Seidel）等其他迭代法要慢。

总结如下：

- **思想**: 在第 `k+1` 次迭代中，计算所有新分量 `xᵢ⁽ᵏ⁺¹⁾` 时，**完全使用**第 `k` 次迭代的旧值 `xⱼ⁽ᵏ⁾`。
- **公式**:
  $$x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)$$
- **特点**:
    - **优点**: 各个分量的计算相互独立，非常容易并行化。
    - **缺点**: 收敛速度通常较慢，且收敛性要求比高斯-赛德尔法更苛刻。

#### 高斯-赛德尔迭代法 (Gauss-Seidel Method)
- **思想**: 在第 `k+1` 次迭代中，计算新分量 `xᵢ⁽ᵏ⁺¹⁾` 时，**立即使用**本轮次已经计算出的新值 `xⱼ⁽ᵏ⁺¹⁾` (j < i)。
- **公式**:
  $$x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right)$$
- **特点**:
    - **优点**: 通常比雅可比法收敛更快。
    - **缺点**: 计算是顺序依赖的，不易并行化。

### 1.3 插值与拟合

目标是根据已知的离散数据点，构造一个连续函数。

#### 1.3.1 拉格朗日插值 (Lagrange Interpolation)
- **思想**: 构造一组“基多项式” `Lᵢ(x)`，每个 `Lᵢ(x)` 在 `xᵢ` 处取值为1，在所有其他的 `xⱼ` 处取值为0。最终的插值多项式就是这些基多项式的加权和。
- **公式**:
  $$P(x) = \sum_{i=0}^{n} y_i L_i(x), \quad \text{其中 } L_i(x) = \prod_{j=0, j \neq i}^{n} \frac{x-x_j}{x_i-x_j}$$
- **特点**:
    - **优点**: 公式结构清晰，易于理解。
    - **缺点**: 计算量大；每增加一个数据点，所有基多项式都需要重新计算。

#### 1.3.2 牛顿插值法 (Newton's Interpolation)

此方法主要为了解决拉格朗日插值“不灵活”的问题。

- **核心思想**:
    采用一种“递推”或“增量”的形式来构造多项式。每增加一个新点，不是推倒重来，而是在原有基础上增加一个“修正项”。

- **核心工具：均差 (Divided Differences)**
    均差是牛顿插值的灵魂，它是一个递归定义的概念。
    - **零阶均差**: $f[x_i] = y_i$
    - **一阶均差**: $f[x_i, x_j] = \frac{f[x_j] - f[x_i]}{x_j - x_i}$
    - **二阶均差**: $f[x_i, x_j, x_k] = \frac{f[x_j, x_k] - f[x_i, x_j]}{x_k - x_i}$
    - **k阶均差**: $f[x_0, \dots, x_k] = \frac{f[x_1, \dots, x_k] - f[x_0, \dots, x_{k-1}]}{x_k - x_0}$
    在计算时，我们通常会列一个**均差表**来系统地算出所有需要的均差值。

- **牛顿插值公式**:
    $P(x) = f[x_0] + f[x_0, x_1](x-x_0) + f[x_0, x_1, x_2](x-x_0)(x-x_1) + \dots$
    
    可以看到，每多一项，都是在前一项的基础上进行补充。

- **特点**:
    - **优点**: 继承了拉格朗日插值的优点（是同一个多项式），但计算上更高效。特别是当需要**增加新的插值点**时，只需在公式末尾追加新的一项即可，非常灵活。
    - **缺点**: 仍然是单一的高次多项式，无法避免龙格现象。

#### 1.3.3 样条插值 (Spline Interpolation)

此方法主要为了解决高次多项式的“振荡”问题。

- **核心思想**:
    放弃使用一个单一的、贯穿所有点的高次多项式。取而代之的是，在每两个相邻的数据点之间，使用一个**低次多项式**（通常是三次，即Cubic Spline），然后通过添加约束条件，让这些分段的多项式在连接点（称为“节点”）处**平滑地拼接**起来。

- **三次样条 (Cubic Spline) 的“平滑”条件**:
    对于每一个内部节点 `xᵢ`，它左右两边的两个三次多项式必须满足：
    1.  **函数值相等 (C⁰连续)**: 保证曲线是连续的。
    2.  **一阶导数值相等 (C¹连续)**: 保证曲线在节点处的**斜率**是连续的，没有尖角。
    3.  **二阶导数值相等 (C²连续)**: 保证曲线在节点处的**曲率**是连续的，形态变化平滑。

- **边界条件**:
    为了唯一确定所有分段三次多项式的系数，还需要在两个端点（`x₀`和`xₙ`）处各补充一个条件。常见的有：
    - **自然样条 (Natural Spline)**: 令两个端点的二阶导数为0。
    - **钳制样条 (Clamped Spline)**: 指定两个端点的一阶导数值。

- **特点**:
    - **优点**: 解决了龙格现象！生成的曲线非常平滑、稳定，是工程和计算机图形学中最常用的插值方法。
    - **缺点**: 最终得到的函数是一个分段函数，而不是一个统一的表达式。计算过程需要求解一个线性方程组。

#### 1.3.4 埃尔米特插值 (Hermite Interpolation)

此方法是一种更高阶的插值，它不仅要求函数曲线穿过数据点，还要求在这些点的“姿态”（斜率）也完全一致。

- **核心思想**:
    构造一个多项式 `P(x)`，它不仅满足 $P(x_i) = y_i$，同时还满足**一阶导数值也相等**，$P'(x_i) = y'_i$。

- **要求**:
    输入的数据不仅包含数据点 `(xᵢ, yᵢ)`，还必须包含这些点的**一阶导数值 `y'ᵢ`**。

- **结果**:
    对于 `n+1` 个数据点，如果我们同时给定了它们的函数值和一阶导数值（共 `2n+2` 个条件），那么可以唯一确定一个次数不超过 `2n+1` 的埃尔米特插值多项式。

- **特点**:
    - **优点**: 由于利用了导数信息，插值函数与原始数据点的“贴合度”非常高，曲线在节点处的形态和趋势都更为精准。
    - **缺点**: **对数据的要求非常苛刻**。在实际问题中，我们往往只能观测到函数值 `yᵢ`，很难获得其精确的导数值 `y'ᵢ`。

### 1.4 数值积分

计算定积分 $\int_a^b f(x)dx$ 的近似值。

#### 梯形法则 (Trapezoidal Rule)
- **思想**: 将积分区间分割成若干小段，用一系列小梯形的面积之和来近似曲线下的总面积。
- **公式 (复合梯形)**:
  $$\int_a^b f(x)dx \approx \frac{h}{2} [f(x_0) + 2\sum_{i=1}^{n-1}f(x_i) + f(x_n)]$$
  其中 `h = (b-a)/n` 是步长。
- **特点**: 简单直观，误差阶为 $O(h^2)$。

#### 辛普森法则 (Simpson's Rule)
- **思想**: 将积分区间分割成若干小段，每**两段**用一条抛物线来拟合曲线，用抛物线下的面积来近似真实面积。
- **公式 (复合辛普森)**:
  $$\int_a^b f(x)dx \approx \frac{h}{3} [f(x_0) + 4\sum_{i=1, i \text{ is odd}}^{n-1}f(x_i) + 2\sum_{i=2, i \text{ is even}}^{n-2}f(x_i) + f(x_n)]$$
  其中 `n` 必须是偶数。
- **特点**: 比梯形法则精度高得多，误差阶为 $O(h^4)$。

### 1.5 常微分方程初值问题

求解形如 `y' = f(x, y)`, `y(x₀) = y₀` 的问题。

#### 欧拉法 (Euler's Method)
- **思想**: 在当前点 `(xₙ, yₙ)`，沿着由 `f(xₙ, yₙ)` 给出的切线方向，前进一小步 `h`，以此来估算下一个点 `yₙ₊₁` 的位置。
- **公式**:
  $$y_{n+1} = y_n + h \cdot f(x_n, y_n)$$
- **特点**:
    - **优点**: 极其简单，是所有更高级方法的基础。
    - **缺点**: 精度很低，误差会随步数累积，不稳定。

### 1.6 精度与误差

数值计算的核心挑战之一是处理**误差**。误差来源主要分为：
1.  **模型误差 (Modeling Error)**: 数学模型本身对现实问题的简化或假设偏差。
2.  **观测误差 (Measurement Error)**: 输入数据（如初始值、参数）的测量不精确。
3.  **截断误差 (Truncation Error)**: 用有限过程（如有限项级数、有限步迭代）近似无限过程（如极限、无穷级数、精确积分）产生的误差。
    *   **示例**: 泰勒展开只取前几项、迭代法未达到精确解即停止。
4.  **舍入误差 (Round-off Error)**: 计算机用有限位数（如单/双精度浮点数）表示实数时产生的误差。计算过程中的四舍五入会累积此误差。

#### 1.6.1 误差分析的核心概念
*   **绝对误差 (Absolute Error)**: 近似值 `x_a` 与真值 `x_t` 之差的绝对值。
    $$E_{abs} = |x_a - x_t|$$
*   **相对误差 (Relative Error)**: 绝对误差与真值绝对值之比（常以百分比表示）。
    $$E_{rel} = \frac{|x_a - x_t|}{|x_t|} \quad (\text{通常要求 } x_t \neq 0)$$
    *   相对误差更能反映近似解的**精度**，尤其在真值很大或很小时。
*   **有效数字 (Significant Digits)**: 近似值 `x_a` 中从第一个非零数字开始，到最后一个能保证准确的数字为止的所有数字。
    *   若 `x_a` 的绝对误差不超过某一位的半个单位，则该位之前的所有数字都是有效数字。
    *   有效数字位数越多，近似解的精度通常越高。

#### 1.6.2 算法的稳定性 (Stability)
*   **定义**: 一个算法是**稳定的**，是指在整个计算过程中，初始数据的小扰动（如舍入误差）对最终结果的影响是可控的（不会导致结果完全失真）。
*   **不稳定的算法**会将计算过程中的小误差急剧放大，导致结果无效。
*   **示例**: 解线性方程组时，系数矩阵接近奇异（条件数大）会导致解对系数或右端项的小扰动极其敏感，此时即使使用精确算法也可能得到错误结果（数值不稳定）。

#### 1.6.3 条件问题 (Conditioning)
*   **定义**: 一个数学问题本身对输入数据扰动的敏感程度称为该问题的**条件数 (Condition Number)**。
    *   条件数大：问题本身是**病态的 (Ill-conditioned)**，输入数据的微小变化会导致解的剧烈变化。
    *   条件数小：问题是**良态的 (Well-conditioned)**，输入数据的扰动对解的影响较小。
*   **关键点**: 问题的病态性是问题本身固有的属性，与使用的算法无关。即使使用最稳定的算法，病态问题也可能得到不准确的结果。
*   **示例**: 求矩阵逆或解线性方程组时，矩阵的条件数 `cond(A) = ||A|| * ||A⁻¹||`（常用范数如 2-范数、∞-范数）是衡量问题病态程度的关键指标。`cond(A)` 越大，问题越病态。

#### 1.6.4 截断误差估计 (以数值积分为例)
*   **复合梯形法则误差**:
    $$E_T = -\frac{(b-a)h^2}{12} f''(\xi), \quad \xi \in [a, b]$$
    其中 `h` 为步长。误差阶为 `O(h²)`。
*   **复合辛普森法则误差**:
    $$E_S = -\frac{(b-a)h^4}{180} f^{(4)}(\xi), \quad \xi \in [a, b]$$
    其中 `h` 为步长，`n`（区间数）为偶数。误差阶为 `O(h⁴)`。
*   **意义**: 这些公式表明，减小步长 `h`（即增加区间数 `n`）可以减小截断误差，且辛普森法则的精度提升速度远快于梯形法则（`h⁴` vs `h²`）。公式中的 `ξ` 是区间内某点，其高阶导数的最大值限制了误差的实际上界。

## 2. 线性代数

### 2.1 空间的基本构成：从向量到空间

#### 张成 (Span)
* **定义**: 一组向量的所有可能的线性组合构成的集合。
* **直观理解**: 给定一组基础向量（积木），它们能“搭建”出的整个几何空间（直线、平面、体）。
* **示例**: `span(α₁, α₂)` 构成一个二维平面。

#### 线性无关 (Linear Independence)
* **定义**: 一组向量中，没有任何一个向量可以被其他向量的线性组合所表示。
* **直观理解**: 每一块“积木”都提供了全新的、不可替代的方向。`α₁`, `α₂`, `α₃` 线性无关，意味着它们分别指向了三个完全不同的维度。

#### 基 (Basis)
* **定义**: 张成整个空间的一组**线性无关**的向量。
* **直观理解**: 搭建整个空间所需要的**最少数量**的“积木”。基向量的数量决定了空间的维度。
* **示例**: `{α₁, α₂, α₃}` 是三维空间的一组基。

#### 维度 (Dimension)
* **定义**: 构成空间的一组基中向量的数量。
* **直观理解**: 描述一个空间有多“大”，需要几个独立的坐标轴才能完全描述。

### 2.2 矩阵的核心功能：空间变换

一个 `m x n` 的矩阵 A，可以看作一个从 n 维空间到 m 维空间的线性变换。

#### 零空间 (Null Space / Kernel)
* **定义**: 所有被矩阵 A 变换为零向量的输入向量 `x` 的集合，即 $A\vec{x} = 0$ 的所有解。
* **直观理解**: 变换中所有被“压扁”到原点的信息。
* **基础解系**: 零空间的一组基。
* **零度 (Nullity)**: 零空间的维度，即基础解系中向量的数量。

#### 秩 (Rank)
* **定义**: 矩阵 A 的列向量张成的空间的维度。
* **直观理解**: 变换后输出空间（像空间）的维度。代表了变换保留了多少维度的信息。
* **计算**: 对矩阵进行高斯消元，得到的阶梯形矩阵中非零行的数量。

#### 秩-零度定理 (The Rank-Nullity Theorem)
* **公式**: `Rank(A) + Nullity(A) = n` (矩阵 A 的**列数**)
* **内在联系**: 输入空间的维度(`n`)，被划分为了两部分：一部分被有效变换（秩），另一部分则被压缩为零（零度）。能量守恒！

### 2.3 变换的深度解析：特征值与特征向量

这是理解一个变换**内在特性**的关键。

#### 特征值 (Eigenvalue, λ) & 特征向量 (Eigenvector, v)
* **定义**: 满足 $A\vec{v} = \lambda\vec{v}$ 的非零向量 $\vec{v}$ 和标量 $\lambda$。
* **直观理解**:
    * **特征向量**: 空间变换中的“主轴”或“不变的方向”。在变换中只发生伸缩，不发生旋转。
    * **特征值**: 在“主轴”方向上的伸缩比例。

#### 特征空间 (Eigenspace, Eλ)
* **定义**: 对应于同一个特征值 `λ` 的所有特征向量，以及零向量，所构成的子空间。
* **直观理解**: 一个“命运共同体”，在这个子空间内的所有向量，都被以同样的比例 `λ` 进行伸缩。它的维度被称为特征值 `λ` 的**几何重数**。
* **示例**: 第五题中，`E₀ = span(α₁, α₂)` 是一个二维特征空间；`E₂ = span(α₃)` 是一个一维特征空间。

### 2.4 可对角化 (Diagonalizability)

* **定义**: 一个 n x n 矩阵 A 如果有 n 个线性无关的特征向量，那么它就是可对角化的。
* **等价条件 (你的直觉)**: 所有特征空间的维度之和，恰好等于矩阵的维度 n。
    * `Σ dim(Eλ) = n`
* **内在联系**: 如果一个矩阵可对角化，意味着它的特征向量足以**张成**整个空间。我们可以切换到这组特征向量构成的“完美坐标系”（基），在这个坐标系下，复杂的变换 A 就变成了一个简单的、只有拉伸作用的对角矩阵 D。这是线性代数中一个极其优美的结论。
* **第五题的升华**: `dim(E₀) + dim(E₂) = 2 + 1 = 3`，等于空间维度。所以该问题中的矩阵是可对角化的。整个空间被特征空间“填满”，没有任何“多余的空位”。因此，任何跨越不同特征空间的向量组合（如D选项），必然不属于任何一个特征空间，故不是特征向量。

### 2.5 实对称矩阵 (Real Symmetric Matrix)

这是性质最丰富、在机器学习中最重要的矩阵类型之一。协方差矩阵就是一个典型的实对称矩阵。

- **定义**: 一个矩阵 `A` 如果满足 $A^T = A$，并且所有元素均为实数，则称其为实对称矩阵。

- **核心性质**:
    1.  **特征值均为实数**: 这是最基本也是最重要的性质。它保证了诸如主成分分析(PCA)中的“方差”这类物理量是真实可测的，而不是复数。
    2.  **不同特征值对应的特征向量相互正交**: 如果 $\lambda_1 \neq \lambda_2$，那么它们对应的特征向量 $\vec{v}_1, \vec{v}_2$ 满足 $\vec{v}_1^T \vec{v}_2 = 0$。
    3.  **可正交对角化**: 综合以上性质，对于任何一个实对称矩阵 `A`，我们一定可以找到一个**正交矩阵** `Q` (满足 $Q^{-1} = Q^T$)，使得：
        $$A = QDQ^T$$
        其中 `D` 是由 `A` 的特征值构成的对角矩阵，`Q` 的列向量是与这些特征值对应的、标准正交化的特征向量。这个过程也叫**谱分解 (Spectral Decomposition)**。

- **与机器学习的联系**:
    - **主成分分析 (PCA)**: PCA的本质就是对数据的协方差矩阵进行谱分解。特征值代表了数据在各个主成分方向上的方差，特征向量就是主成分的方向。

### 2.6 SVD 分解 (Singular Value Decomposition)

SVD是线性代数中的“瑞士军刀”，它强大到可以应用于**任何 `m x n` 的矩阵**，无论方阵、非方阵、满秩、亏秩。

- **定义**: 任何矩阵 `A` 都可以分解为：
    $$A_{m \times n} = U_{m \times m} \Sigma_{m \times n} V_{n \times n}^T$$

- **核心构成**:
    - **`U` (左奇异向量)**: 一个 `m x m` 的**正交矩阵**。其列向量是 $AA^T$ 的特征向量。在数据分析中，`U` 的列通常与“行”的某种主题或概念相关（例如，用户主题）。
    - **`Σ` (奇异值)**: 一个 `m x n` 的**对角矩阵**（准确说是伪对角），对角线上的元素 $\sigma_i$ 称为**奇异值**。它们非负，并按从大到小的顺序排列。奇异值是 $A^TA$ (或 $AA^T$) 的特征值的平方根。
    - **`V` (右奇异向量)**: 一个 `n x n` 的**正交矩阵**。其列向量是 $A^TA$ 的特征向量。在数据分析中，`V` 的列通常与“列”的某种主题或概念相关（例如，物品主题）。

- **重要性质与直观理解**:
    - **几何意义**: SVD将任意一个线性变换分解为三个基本动作：一个**旋转** ($V^T$)、一个沿坐标轴的**缩放** ($\Sigma$)、再加另一个**旋转** ($U$)。
    - **唯一性**: 对于任意矩阵，其**奇异值是唯一确定的**。但左右奇异向量不唯一（例如，对应的列向量同时乘以-1仍然成立）。这正是之前第9题的考点。
    - **低秩近似**: SVD最重要的应用。奇异值的大小代表了数据中对应“模式”的重要性。我们可以只保留最大的 `k` 个奇异值及其对应的奇异向量来近似原始矩阵，这在**降维、数据压缩、去噪**中是核心技术。

- **与机器学习的联系**:
    - **推荐系统**: 在用户-物品评分矩阵中，用SVD进行矩阵分解，发现用户和物品的潜在因子。
    - **自然语言处理**: 潜在语义索引 (LSI) 技术就是基于SVD。
    - **图像处理**: 图像压缩。

### 2.7 QR 分解 (QR Decomposition)

QR分解常用于数值计算，特别是求解线性方程组和特征值问题。

- **定义**: 任何满秩的 `m x n` 矩阵 `A` 都可以分解为：
    $$A = QR$$

- **核心构成**:
    - **`Q`**: 一个 `m x m` 的**正交矩阵**。
    - **`R`**: 一个 `m x n` 的**上三角矩阵**。

- **直观理解**:
    - QR分解的本质是**施密特正交化 (Gram-Schmidt Orthogonalization)** 的稳定实现。
    - 我们可以把矩阵 `A` 的列向量看作空间中的一组基。QR分解的过程就是把这组普通的基，变成一组**标准正交基**（`Q`的列向量），而 `R` 矩阵则记录了如何通过这组新的标准正交基来表示回原始的基。

- **与机器学习的联系**:
    - **最小二乘法求解**: 在解线性回归 $A\vec{x} = \vec{b}$ 时，如果 `A` 不是方阵，通常求解正规方程 $A^TA\vec{x} = A^T\vec{b}$。使用QR分解可以更稳定、更高效地求解。

### 2.8 LU 分解 (LU Decomposition)

LU分解主要用于求解线性方程组和求逆矩阵，它本质上是高斯消元法的矩阵形式。

- **定义**: 一个**方阵** `A` 如果满足一定条件，可以分解为：
    $$A = LU$$

- **核心构成**:
    - **`L`**: 一个**下三角矩阵 (Lower triangular)**，对角线元素为1。
    - **`U`**: 一个**上三角矩阵 (Upper triangular)**。

- **直观理解**:
    - **高斯消元法的“记账本”**。`U` 矩阵就是我们辛辛苦苦通过行变换进行高斯消元后得到的最终结果（阶梯形矩阵）。而 `L` 矩阵则像一个账本，记录了我们消元过程中所有的操作步骤（比如“第二行减去第一行的2倍”，这个“2”就会被记在`L`的对应位置）。

- **存在性**:
    - LU分解**并非对所有方阵都存在**。当高斯消元过程中出现主元（对角线元素）为0，导致无法继续时，就需要进行“行交换”。加入了行交换的分解称为 `PA = LU` 分解，其中 `P` 是一个记录了行交换操作的置换矩阵。

- **与机器学习的联系**:
    - **高效求解线性方程组**: 当需要对同一个矩阵 `A` 和多个不同的向量 `b` 求解 `Ax=b` 时，LU分解的优势巨大。我们只需对 `A` 分解一次，然后通过简单的向前和向后代入法（解 $L\vec{y}=\vec{b}$ 和 $U\vec{x}=\vec{y}$）即可快速求出解，避免了重复进行高斯消元或求逆的巨大计算量。

## 2# 线性代数几何直观

这份笔记旨在超越公式，构建一个关于线性代数核心思想的几何与物理直觉。

### 2#.1 矩阵的本质：空间的变换场

矩阵不仅是数字的网格，其本质是一个**线性变换 (Linear Transformation)**。我们可以将其想象成一个定义在整个空间中的**矢量场**，它规定了空间中每一个点 `x` 将如何“流动”到新的点 `Ax`。一个具象的可视化解释就是机器学习当中的前馈神经网络/多层感知机。

- **几何图像**: 矩阵的列向量，精准地揭示了标准坐标系的基向量（单位向量 `i, j, k`）变换后的新位置。整个空间的扭曲、缩放、旋转，都是由这几个基向量的“命运”所决定的。
- **维度升降**:
    - **满秩 (Full Rank)**: 矩阵的列向量线性无关。变换保留了空间的维度，仅仅是“扭曲”了它。
    - **亏秩 (Rank Deficient)**: 矩阵的列向量线性相关。说明向量组存在“重复”的基，即缺少支撑原始维度的基，变换会将原始空间“压扁”或“坍缩”到一个更低维的子空间（即矩阵的列空间/值域）中。那些被“压扁”到零点的向量，构成了矩阵的**零空间 (Null Space)**。

### 2#.2 变换的深度解析：寻找不变的“主轴”

在复杂的空间流动中，我们渴望找到其内在的稳定结构。**特征向量 (Eigenvectors)** 和 **特征值 (Eigenvalues)** 就是这个稳定结构的核心。线性变换可以拆解为两个部分：旋转/镜像与剪切，即对坐标系的变换，以及缩放，即“相似”概念中描述的“不同坐标系下的同一个变换”的变换。前者表现为特征向量/特征空间，后者表现为特征值。

- **特征向量**: 构成变换场中“主轴”或“不变方向”的基本单元。位于这些方向上的向量，在变换中只会被缩放，**不会发生方向的偏转（旋转/镜像或剪切）**。
- **特征值**: 对应主轴方向上的缩放比例，是线性变换抛开坐标系视角后的变换本身。
- **特征空间 (Eigenspace)**: 由同一特征值对应的所有特征向量构成的子空间，描述了“主轴”的完整实体。它是一个**不变子空间**，内部的任何向量在变换后，结果仍然留在这个子空间内。

### 2#.3 理想世界：可对角化的变换

当一个n维变换拥有n个线性无关的特征向量时，我们就处在一个“理想世界”中。这意味着它的特征空间足以**张成 (Span)** 整个n维空间。

- **相似**：特征值是一切线性变换的特征，这就是说拥有相同特征模式的矩阵实际上是在不同的坐标系视角下进行相同的变换，即满足 $P^{-1}AP=B$ 则说明 $A\sim B$。要强调的是，“特征模式相同”不仅要求特征值相同，特征值的几何重数也应该相同，几何重数在这里扮演着各个特征值对整个特征空间维度的分配与统治关系。

- **相似对角化 ($A = PDP^{-1}$)**: 这不是一个单纯的公式，而是一种**视角的切换**。
    - **`A`**: 变换在标准坐标系下的（通常是复杂的）描述。
    - **`P`**: 一个“坐标系翻译官”（由特征向量作列构成），负责将向量从“特征坐标系”翻译回“标准坐标系”。
    - **`D`**: 同一个变换，在“特征坐标系”下的（极其简单的）描述。它是一个对角矩阵，其特征空间基底是独热的，其元素就是特征值，这就是说，对角阵是一种没有旋转/镜像与剪切，仅缩放的线性变换，这完美地揭示了变换的本质——**纯粹的、沿着主轴的缩放**。

- **正交矩阵**：正交矩阵是一种特殊的可逆矩阵，它的向量组之间两两正交且模长为1，这种矩阵的实特征值的绝对值为1，这说明正交矩阵没有缩放，是纯粹的各向同性的旋转/镜像的表现。

- **正交对角化**：谱定理告诉我们，一个实矩阵 $A$ 可以被正交对角化的充要条件是：$A$ 是实对称矩阵。这就是说，实对称矩阵是一种在正交空间中进行缩放变换的线性变换，可以通过对应的正交阵进入这个正交空间。对于坐标系变换不满足各向同性要求的可对角化的矩阵，其 **`P`** 矩阵不可为正交矩阵。

### 2#.4 现实世界的复杂性：剪切现象

如果说纯粹缩放表现为对角阵，纯粹旋转/镜像（各向同性）表现为正交阵，那么对剪切操作的解释则相对复杂很多。当一个n维变换的线性无关特征向量不足n个时（即某特征值的**几何重数 < 代数重数**），理想世界被打破，复杂的**剪切 (Shear)** 现象登场。

#### 剪切的双重视角

对于剪切矩阵 $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$，我们可以从两个视角去理解：

- **视角一：原始矩阵 $A$ (全局/现象视角)**
    我们观察到的是变换的最终效果。在这里，我们发现了一个奇特的“**分量守恒**”现象：输入向量 `[x, y]` 变换为 `[x+y, y]`，其 **y分量的值保持不变**。这是剪切变换在全局坐标系下表现出的代数属性，这种对特定维度“不干涉”的现象是特征空间维数缺损在变换场域中的直接表现。

- **视角二：算子 $A - λI$ (诊断/机理视角)**
    这是深入变换内部的“诊断工具”。对于剪切矩阵，其唯一的特征值是 $λ=1$。我们考察 $A - I = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$。这个算子滤除了“1倍缩放”的良性行为，暴露了其“病灶”：它将y轴方向的向量 `[0, y]` “滑动”并“拍扁”到了x轴上 `[y, 0]`。这揭示了特征空间维数缺损的**内部动力学机制**的表现。

#### 广义特征空间与变换链条

“消失”的特征空间维度并没有蒸发，而是与仅存的特征空间纠缠在一起，形成了**广义特征空间 (Generalized Eigenspace)**。

- **变换链条**: 在剪切矩阵的广义特征空间内，向量不再是平等的。它们形成了一个“等级链条”。
    - **贵族 (普通特征向量 `v`)**: 位于x轴，满足 `(A-I)v = 0`，享受纯粹的1倍缩放。
    - **仆人 (广义特征向量 `u`)**: 例如y轴向量，它不再满足 `(A-I)u=0`，而是满足 `(A-I)u = v`。

- **剪切与特征值的绑定**:
    这个链条关系 `Au - u = v` 可以写为：
    $$Au = 1 \cdot u + v$$
    这个公式是理解剪切与特征值绑定的钥匙。它表明，对广义特征向量 `u` 的变换，由两部分构成：
    1.  一部分是**与其绑定的特征值 `λ=1` 所定义的缩放** (`1 * u`)。
    2.  另一部分是一个**额外的、沿着“贵族”特征向量 `v` 方向的“推移”**，这就是**剪切**。

剪切，本质上是一个重根特征值无法完全实现其所有维度的纯粹缩放，从而在内部产生的一种“补偿性”的、等级化的复杂变换。整个故事（缩放、滑动、纠缠）都发生在与这一个特征值`λ`相关联的广义特征空间内部，因此两者严格绑定。**约旦标准型 (Jordan Form)** 则是将矩阵写成这种“一个特征值，一个故事”的最终形式。

### 2#.5 奇异值分解（SVD）：线性变换的大一统模型

对于任何一个线性变换（无论它是否会改变空间的维度），我们能否找到一组‘最完美’的标准正交坐标系，使得变换在这组坐标系之间的作用，仅仅是纯粹的缩放？谱分解（特征值分解）给出了一个直观但不通用的答案，而奇异值分解则认为，对于任何矩阵，我们都能找到两个这样的坐标系：一个用于输入空间，一个用于输出空间。

特征值分解问的是一个向内的问题，它关心的是变换的内在结构和稳定性，特征向量就是这个结构中“不变”的核心。如果一个变换的内在结构本身就是“拧巴”的（比如剪切矩阵），它的某些特征空间维度被隐藏起来了，成为了广义特征空间，那么这种“内省”式的剖析就会失败，这就是不可对角化。

要从特征值走向奇异值，我们仅需转换一个视角：从追求 $Ax=\lambda x$ 中 $x$ 的前后一致性，到追求 $Av=\sigma u$ 中 $\sigma$ 可能的最大值。假设 $A^TAv=\lambda v$，则存在谱分解：$A^TA=V\Lambda V^T$，令 $\Sigma=\sqrt{\Lambda},\ U=AV\Sigma^T$，则 $U^TU=\Sigma V^TA^TAV\Sigma^T=\Sigma V^TV\Lambda V^TV\Sigma^T=I$，说明 $A=U\Sigma V^T$ 表示 $A$ 一定能分解为两个正交阵和一个对角阵的形式。

- **转置**：矩阵转置是一个比较初级的概念，不过要清楚地理解它的几何直观，我们不得不绕一大圈，从SVD的角度去分析。
对于任意矩阵我们有：$A=U\Sigma V^T$，假设它是可逆的，则它的逆矩阵则是：$A^{-1}=(V^T)^{-1}\Sigma^{-1}U^{-1}=V\Sigma^{-1}U^T$，其转置则是：$A^T=(V^T)^T\Sigma^TU^T=V\Sigma U^T$，也就是说，矩阵的转置和矩阵的逆的唯一差别，在于转置没有像反转旋转变换那样反转缩放变换，保持了与原矩阵相同的缩放变换。

- **奇异值分解**：理解奇异值分解的代数几何直观，最好的方式是从得证的结论出发，回溯性地分析它和特征值分解（谱分解）的差别。

    - **$A^TA$与$AA^T$**：对于任意可逆矩阵我们有：$A=U\Sigma V^T$，沿用转置讨论中得到的结论，那么 $A^TA=(V\Sigma U^T)(U\Sigma V^T)=V\Sigma^2V^T$，同理 $AA^T=U\Sigma^2 U^T$，就是说，$A^TA$ 和 $AA^T$ 的第一种解读是“中和掉了矩阵 $A$ 的旋转操作，但保留了两次缩放操作”；第二种解读是“只进行‘入场/出场’的正逆变换，抵达中转空间后缩放一下就回去了”。这说明了，$A^TA$ 和 $AA^T$ 一定是半正定（特征值是奇异值的平方）实对称矩阵（可以被正交对角化）。
    - **谱分解**：谱分解的核心观念是：“线性变换的本质是不同视角下的缩放”，所以谱分解存在着一个对称的视角转换。但是，这种观念假设了目标矩阵不隐式表达了旋转操作。换句话说，谱分解的两个对称操作表示了视角的往复，但是假设视角变换的终点并不是回到原地呢？
    - **极分解**：极分解的形式非常简单：$A=RS$，即任何可逆方阵 $A$ 都可以分解为一个旋转（正交矩阵$R$）和一个对称拉伸（对称矩阵$S$）的乘积。也就是说，$A=U\Sigma V^T$，其中 $S=V\Sigma V^T$，$R=UV^T$，即 $U=RV$。极分解解释了实对称矩阵和其它矩阵的差距，而这个差距正是谱分解所疏忽的“纯粹旋转”。也就是说，正交阵自身作为一种“纯粹旋转”，是实对称矩阵的绝对反面。

    任何可逆矩阵的奇异值分解均有唯一的极分解形式，对于不可逆矩阵来说，奇异值分解的双正交阵约束了整个线性变换的趋势，而极分解的旋转矩阵则走向了自由和不唯一；对于非方阵来说，传统意义上的极分解失效了，但如果存在一种广义上的极分解，其中 $S$ 是 $n\times n$ 的对称半正定矩阵，$P$ 是一个 $m\times n$ 的等距同构矩阵，那么 $S=\sqrt{A^TA}=V\sqrt{\Sigma^T\Sigma}V^T$，$P=UV^T$，最终仍会退行回奇异值分解的形式 $A=U\Sigma V^T$。

    综上所述，奇异值分解通过解耦线性变换，实现了某种大一统的线性代数分析法。

## 3. 概率论

### 3.1 概率论基础 (The Foundations)

这是后续所有知识的“公理”和“语法”。

#### 3.1.1 随机事件与概率
- **基本概念**:
    - **样本空间 (Ω)**: 所有可能结果的集合。
    - **随机事件 (A, B)**: 样本空间的一个子集。
- **核心公式**:
    - **加法公式**: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
    - **条件概率**: 在B发生的前提下，A发生的概率。$P(A|B) = \frac{P(A \cap B)}{P(B)}$
    - **乘法公式**: $P(A \cap B) = P(A|B)P(B) = P(B|A)P(A)$
    - **全概率公式**: $P(A) = \sum_{i=1}^{n} P(A|B_i)P(B_i)$ (将复杂事件分解为多种简单情况)
    - **贝叶斯公式 (Bayes' Theorem)**:
      $$P(B_i|A) = \frac{P(A|B_i)P(B_i)}{\sum_{j=1}^{n} P(A|B_j)P(B_j)}$$
      (核心应用：后验概率的计算，是贝叶斯统计和很多机器学习算法的基础)

### 3.2 随机变量及其数字特征

这是将随机事件“数学化”和“量化”的关键。

#### 3.2.1 随机变量 (Random Variable)
- **离散型 (Discrete)**: 取值有限或可数。由**概率质量函数 (PMF)** $P(X=x_k)=p_k$ 描述。
- **连续型 (Continuous)**: 取值在某个区间内不可数。由**概率密度函数 (PDF)** $f(x)$ 描述，满足 $f(x) \ge 0$ 且 $\int_{-\infty}^{\infty}f(x)dx=1$。
- **分布函数 (CDF)**: $F(x) = P(X \le x)$，对离散和连续型都通用。

#### 3.2.2 期望与方差的代数特性
这是**必考**重点，需要熟练运用。

- **期望 (Expectation, E[X])**: 概率意义下的“均值”，描述随机变量的**集中趋势**。
    - **性质**:
        - `E[c] = c` (c为常数)
        - `E[cX] = cE[X]`
        - **`E[X + Y] = E[X] + E[Y]`** (期望的线性性，**不要求**X, Y独立)
        - `E[XY] = E[X]E[Y]` (**要求**X, Y相互独立)

- **方差 (Variance, Var(X))**: 描述随机变量取值相对于其期望的**离散程度**。
    - **定义**: `Var(X) = E[(X - E[X])²] = E[X²] - (E[X])²` (计算方差的常用公式)
    - **性质**:
        - `Var(c) = 0`
        - `Var(X + c) = Var(X)`
        - `Var(cX) = c²Var(X)`
        - **`Var(X + Y) = Var(X) + Var(Y)`** (**要求**X, Y相互独立)
        - `Var(X - Y) = Var(X) + Var(Y)` (**要求**X, Y相互独立)

- **协方差与相关系数**:
    - **协方差 (Covariance)**: `Cov(X, Y) = E[(X-E[X])(Y-E[Y])]`，描述两个变量的线性相关方向。
    - **相关系数 (Correlation)**: $\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$，取值[-1, 1]，描述线性相关强度。

### 3.3 常见概率分布

这是将理论模型应用到现实问题的核心，**判断事件服从什么分布**的能力至关重要。

#### 3.3.1 常用离散分布

1.  **0-1分布 / 伯努利分布 (Bernoulli)**
    - **描述**: 单次随机试验，结果只有两种（成功/失败）。
    - **典型场景**: 抛一次硬币的结果；一件产品是否合格。
    - $P(X=k) = p^k(1-p)^{1-k}$, k=0,1. E[X]=p, Var(X)=p(1-p).

2.  **二项分布 (Binomial, B(n, p))**
    - **描述**: n次**独立重复**的伯努利试验中，成功的次数。
    - **典型场景**: 抛10次硬币，正面朝上恰好3次的概率；射击10次，命中恰好8次的概率。
    - $P(X=k) = C_n^k p^k (1-p)^{n-k}$. E[X]=np, Var(X)=np(1-p).

3.  **泊松分布 (Poisson, P(λ))**
    - **描述**: 单位时间/空间内，某事件发生的次数。
    - **典型场景**: 一小时内到达某路口的车辆数；一页书中印刷错误的个数；一个网站每分钟收到的访问次数。
    - $P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}$. E[X]=λ, Var(X)=λ.
    - **与二项分布关系**: 当n很大，p很小时，二项分布近似于泊松分布（λ=np）。

4.  **几何分布 (Geometric)**
    - **描述**: 在一系列独立重复的伯努利试验中，**首次**成功所需要的试验次数。
    - **典型场景**: 不停地抛硬币，直到第一次出现正面为止，所需要的次数。
    - E[X]=1/p.

#### 3.3.2 常用连续分布

1.  **均匀分布 (Uniform, U(a, b))**
    - **描述**: 在区间 `[a, b]` 内，随机变量取任何值的概率都相等。
    - **典型场景**: 抽奖转盘停在任意角度的概率；一个在[0,1]之间均匀生成的随机数。
    - $f(x) = \frac{1}{b-a}$ for a≤x≤b. E[X]=$\frac{a+b}{2}$, Var(X)=$\frac{(b-a)^2}{12}$.

2.  **指数分布 (Exponential)**
    - **描述**: 泊松过程中，两个事件发生之间的**时间间隔**。
    - **典型场景**: 呼叫中心接到下一个电话需要等待的时间；一个灯泡能正常工作的寿命。
    - **关键特性**: **无记忆性**。`P(X > s+t | X > s) = P(X > t)`。
    - $f(x) = \lambda e^{-\lambda x}$ for x>0. E[X]=1/λ, Var(X)=1/λ².

3.  **正态分布 / 高斯分布 (Normal, N(μ, σ²))**
    - **描述**: 自然界和工程中大量现象的分布规律，由均值`μ`和方差`σ²`决定。
    - **典型场景**: 大量人口的身高、体重；测量误差；大量独立随机变量之和（中心极限定理）。
    - **重要特性**:
        - **标准正态分布 N(0, 1)**: `μ=0, σ=1`。任何正态分布都可以通过 `Z = (X-μ)/σ` 进行标准化。
        - 3σ法则：约99.7%的数据落在`(μ-3σ, μ+3σ)`区间内。

### 3.4 数理统计的桥梁：重要定理

- **大数定律 (Law of Large Numbers)**: 当样本量足够大时，样本均值会收敛于真实期望。这是用采样估计总体的理论基础。
- **中心极限定理 (Central Limit Theorem, CLT)**: 无论原始总体是什么分布，只要样本量`n`足够大，**样本均值的分布**都将近似于**正态分布**。这是正态分布“称霸”统计学的根本原因，也是很多统计检验的理论基石。

### 3.5 推断统计：假设检验

这是利用样本信息来判断关于总体的某个假设是否成立的科学方法。

#### 3.5.1 假设检验的基本逻辑
1.  **建立假设**:
    - **原假设 H₀**: 通常是想要**拒绝**的、保守的、无变化的假设（如“药物无效”、“均值相等”）。
    - **备择假设 H₁**: 我们希望**接受**的、有变化的假设（如“药物有效”、“均值不等”）。
2.  **选择检验统计量**: 根据问题类型选择一种检验方法（如T检验）。
3.  **确定显著性水平α**: 事先设定的犯“弃真”错误的概率上限，通常为0.05或0.01。
4.  **计算p值**: 假设H₀为真时，观测到当前样本结果或更极端结果的概率。
5.  **做出决策**:
    - **如果 p值 < α**: 认为是小概率事件发生了，我们有充分理由**拒绝原假设H₀**。
    - **如果 p值 ≥ α**: 证据不足，我们**不能拒绝原假设H₀**。

#### 3.5.2 常用检验方法

- **正态性检验 (Normality Test)**
    - **目的**: 检验一组样本数据是否来自一个服从正态分布的总体。这是很多其他统计检验（如T检验）的前提。
    - **常用方法**: Shapiro-Wilk检验、Kolmogorov-Smirnov (K-S)检验。
    - **应试理解**: 知道它的**目的**是检验数据是否为正态分布即可。

- **显著性差异检验 (Significance Test)**
    - **目的**: 检验两个或多个组之间，某个指标（通常是均值）的差异是否是“显著的”，而非由随机抽样误差造成的。
    - **最常用方法：T检验 (t-test)**:
        - **适用场景**: 样本量较小（如n<30）且总体方差未知时，检验**一个或两个样本均值**的差异。
        - **类型**:
            - **单样本T检验**: 检验单个样本的均值是否等于一个已知的目标值。
            - **独立双样本T检验**: 检验**两个独立组**的均值是否存在显著差异。（例如：比较A/B测试中两个不同版本广告的点击率均值）
            - **配对样本T检验**: 检验**同一个对象**在两种不同处理前后的均值是否存在显著差异。（例如：检验同一批病人在服药前后的血压均值差异）

## 4. 支持向量机 (SVM)

SVM (Support Vector Machine) 是一种强大的监督学习模型，主要用于分类和回归。其核心思想是找到一个最优的决策边界，使得两类样本之间的间隔（Margin）最大化。

### 4.1 线性可分与硬间隔 SVM

这是最理想的情况，假设两类数据点可以被一条直线（或一个平面）完美地分开。

#### 核心目标：找到最宽的“街道”

- **超平面 (Hyperplane)**: 在二维空间中就是一条直线，三维中是一个平面。它是我们的决策边界。
- **间隔 (Margin)**: 离超平面最近的、分属两类的点（即支持向量）到超平面的距离之和。这就是“街道”的宽度。
- **支持向量 (Support Vectors)**: 那些“贴在街道边缘”上的点，它们是定义决策边界的关键先生。

#### 公式解析

1.  **超平面方程**:
    $$w^T x + b = 0$$
    - `w`: 法向量，决定了超平面的**方向**。
    - `x`: 特征向量。
    - `b`: 偏置项，决定了超平面**在空间中的位置**。

2.  **决策函数**:
    $$f(x) = \text{sign}(w^T x + b)$$
    通过计算结果的符号（+1 或 -1）来判断一个点 `x` 属于哪一类。

3.  **最大化间隔**:
    “街道”的两条边界可以定义为 $w^T x + b = 1$ 和 $w^T x + b = -1$。这两条边界之间的距离，也就是间隔宽度，可以推导为：
    $$\text{Margin} = \frac{2}{\|w\|}$$
    我们的目标是**最大化**这个间隔，这等价于**最小化** `||w||`，为了数学计算方便，我们通常最小化 $\frac{1}{2}\|w\|^2$。

4.  **约束条件**:
    所有点都必须被正确分类，并且不能进入“街道”内部。
    $$y_i(w^T x_i + b) \ge 1 \quad (\text{对于所有样本 } i)$$
    - `yᵢ`: 样本 `i` 的标签（+1 或 -1）。
    - `xᵢ`: 样本 `i` 的特征向量。

**硬间隔SVM的优化问题总结如下：**
$$
\min_{w,b} \frac{1}{2}\|w\|^2 \quad \text{s.t.} \quad y_i(w^T x_i + b) \ge 1, \forall i
$$

### 4.2 应对现实：线性不可分与软间隔 SVM

现实数据往往是嘈杂的，总有几个“离群点”导致数据线性不可分。硬间隔SVM会因此找不到解。

#### 核心目标：允许犯错，但要付出代价

我们引入一个**松弛变量 (Slack Variable)** $\xi_i \ge 0$ (读作 "k'sai")，允许某些点可以“越界”。
- 如果 $\xi_i = 0$：该点严格遵守规则。
- 如果 $0 < \xi_i \le 1$：该点在间隔内，但仍在正确的一侧。
- 如果 $\xi_i > 1$：该点被错误分类。

#### 公式解析

1.  **新的约束条件**:
    $$y_i(w^T x_i + b) \ge 1 - \xi_i \quad (\text{对于所有样本 } i)$$

2.  **新的优化目标**:
    我们既要最小化 $\frac{1}{2}\|w\|^2$ (保持街道尽可能宽)，又要最小化总的“犯错程度” $\sum \xi_i$。
    $$
    \min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{n} \xi_i
    $$

3.  **参数 `C` (正则化系数) 的解析**:
    `C` 是一个超参数，用来平衡“街道宽度”和“犯错代价”之间的**权衡 (Trade-off)**。
    - **`C` 值很大**: 意味着对“犯错”（$\xi_i>0$）的**惩罚极高**。模型会尽力减小 $\xi_i$，哪怕牺牲间隔宽度，也要把大部分点都分类正确。这会导致决策边界变得复杂，容易**过拟合**。
    - **`C` 值很小**: 意味着对“犯错”的**容忍度很高**。模型会更倾向于保持一个很宽的间隔，哪怕因此多错分了几个点。这会导致决策边界比较简单平滑，泛化能力可能更好，但也可能**欠拟合**。

### 4.3 终极武器：非线性问题与核技巧

如果数据分布本身就是弯曲的（例如，一圈点在内，一圈点在外），线性分类器完全无效。

#### 核心目标：升维打击，在高维空间中线性分割

- **核技巧 (The Kernel Trick)**: 它的思想是，我们通过一个非线性函数 $\phi(x)$ 将原始数据映射到一个更高维的空间，在那个新空间里，数据可能就变得线性可分了。
- 核技巧的精髓在于，我们**不需要**真正地计算这个复杂的映射 $\phi(x)$，而是通过一个**核函数 (Kernel Function)** $K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$ 来直接计算高维空间中的点积。

#### 公式解析

1.  **RBF核函数 (径向基函数核)**:
    这是最流行的一种核函数。
    $$K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$$

2.  **参数 `γ` (gamma) 的解析**:
    `γ` 定义了单个训练样本的“影响力”范围。
    - `||xᵢ - xⱼ||²`: 两个点之间距离的平方。
    - **`γ` 值很大**: 指数项会随着距离的增加而**急剧衰减**。这意味着只有非常近的点才会对彼此产生影响。决策边界会变得非常“崎岖”，紧紧地围绕着数据点，模型**非常复杂**，极易**过拟合**。
    - **`γ` 值很小**: 指数项会随着距离的增加而**缓慢衰减**。这意味着一个点可以影响到很远的其他点。决策边界会因此变得非常“平滑”，模型**比较简单**，可能**欠拟合**。

---

### 4.4 快速总结备忘

| 参数 | 值增大 $\uparrow$ | 值减小 $\downarrow$ |
| :--- | :--- | :--- |
| **C** | 惩罚变重，模型更复杂，易**过拟合** | 容忍度高，模型更简单，易**欠拟合** |
| **γ** | 影响半径小，模型更复杂，易**过拟合** | 影响半径大，模型更简单，易**欠拟合** |

## 5.主成分分析 (PCA) 

PCA是一种广泛应用的无监督学习算法，其核心目标是**数据降维 (Dimensionality Reduction)** 和**特征提取 (Feature Extraction)**。

### 5.1 核心目标：信息浓缩与降维

在高维数据中（例如，一张图片有上万个像素特征），特征之间常常存在高度相关性，这意味着信息是冗余的。PCA的目标就是消除这种冗余，找到一组新的、彼此无关的坐标轴（称为“主成分”），并用少数几个新的坐标轴来表示原始数据，同时尽可能多地保留原始数据中的信息。

- **直观理解**: 把一滩“杂乱无章”的数据点，旋转到一个“最佳视角”，使得数据在这个视角下看起来最“分散”（方差最大），从而最能体现其主要结构。

### 5.2 PCA的核心思想：最大方差理论

“信息量”在PCA中被定义为**方差 (Variance)**。一个特征的方差越大，说明它在数据集中变化越大，包含的信息就越多。

- **PCA的目标**: 寻找到一个新的、由一组标准正交基构成的坐标系。其中：
    - **第一主成分 (PC1)**: 是原始数据中**方差最大**的那个方向。
    - **第二主成分 (PC2)**: 是与PC1**正交**的前提下，方差次大的方向。
    - **第三主成分 (PC3)**: 是与PC1和PC2都**正交**的前提下，方差再次大的方向。
    - ...以此类推。

通过保留前 `k` 个方差最大的主成分，我们就得到了一个能最大程度保留原始信息（方差）的 `k` 维数据表示。

### 5.3 数学基石：协方差矩阵的特征值分解

这个“寻找最大化方差的正交方向”的优化目标，在数学上完美地等价于**对数据的协方差矩阵进行特征值分解**。

- **协方差矩阵 (Covariance Matrix) `C`**:
    - 一个实对称矩阵。
    - 其对角线元素是各个原始特征的方差。
    - 其非对角线元素是不同原始特征之间的协方差。

- **核心联系**:
    - **协方差矩阵的特征向量 (Eigenvectors)**: 正是我们要寻找的**主成分方向**。因为协方差矩阵是实对称的，这些特征向量**天然就是正交的**！
    - **协方差矩阵的特征值 (Eigenvalues)**: 恰好是数据在对应主成分方向（特征向量）上的**方差大小**。特征值越大，说明该主成分越重要。

这个美妙的对应关系，让我们之前讨论的“实对称矩阵”和“特征值分解”的知识有了用武之地。

### 5.4 PCA的计算步骤

假设我们有一个 `n x p` 的数据矩阵 `X`（n个样本，p个特征）。

1.  **数据中心化 (Data Centering)**
    对每一个特征（每一列），减去该特征的均值。
    $$X' = X - \text{mean}(X)$$
    **目的**: 确保我们分析的是数据围绕其中心的散布情况，而不是围绕坐标原点。

2.  **计算协方差矩阵 `C`**
    $$C = \frac{1}{n-1} X'^T X'$$
    这是一个 `p x p` 的实对称矩阵。

3.  **对协方差矩阵 `C` 进行特征值分解**
    计算出所有的特征值 $\lambda_1, \lambda_2, \dots, \lambda_p$ 和对应的特征向量 $\vec{v}_1, \vec{v}_2, \dots, \vec{v}_p$。

4.  **排序并选择主成分**
    将特征值按从大到小的顺序排列。对应的特征向量也随之排序。最大的特征值 $\lambda_1$ 对应的特征向量 $\vec{v}_1$ 就是第一主成分 (PC1)，以此类推。

5.  **转换到新空间（降维）**
    选择前 `k` 个最重要的主成分（即前k个特征向量），将它们作为列向量，组成一个 `p x k` 的变换矩阵 `W`。
    $$W = [\vec{v}_1, \vec{v}_2, \dots, \vec{v}_k]$$
    用原始的中心化数据 `X'` 乘以这个变换矩阵，就得到了降维后的 `n x k` 新数据矩阵 `Z`。
    $$Z = X'W$$

### 5.5 PCA与SVD的联系 (更高效的实现方式)

在实践中，尤其是当特征维度 `p` 非常高时，计算 `p x p` 的协方差矩阵并对其进行特征值分解的计算成本很高。这时，SVD提供了一个更直接、数值更稳定的方法。

对**中心化后**的数据矩阵 `X'` 直接进行SVD分解：
$$X'_{n \times p} = U_{n \times n} \Sigma_{n \times p} V_{p \times p}^T$$

我们之前推导过，`X'^T X' = V\Sigma^T\Sigma V^T`。这正是协方差矩阵的特征值分解形式！

- **核心联系**:
    - SVD的**右奇异向量** (矩阵`V`的列) 就是我们要找的**主成分**（等价于协方差矩阵的特征向量）。
    - **奇异值的平方** (`σᵢ²`) 与协方差矩阵的**特征值**成正比（$\lambda_i = \frac{\sigma_i^2}{n-1}$）。

因此，通过对数据矩阵`X'`进行SVD，我们可以直接得到主成分`V`和相关的方差信息，从而绕过了计算庞大的协方差矩阵这一步。

### 5.6 实践中的考量：如何选择`k`？

- **解释方差比 (Explained Variance Ratio)**
    计算每个主成分所解释的方差占总方差的比例 $\frac{\lambda_i}{\sum \lambda_j}$，然后看前 `k` 个主成分的累计方差贡献率是否达到一个阈值（例如95%或99%）。
- **碎石图 (Scree Plot)**
    将排序后的特征值（方差）绘制成条形图。通常图形会有一个明显的“肘部”，即特征值从大到小急剧下降后变得平缓的拐点。这个拐点之后的主成分通常可以被忽略。

## 6. 聚类算法

### 6.1 什么是聚类分析？

聚类 (Clustering) 是一种经典的**无监督学习**任务。

* **核心目标**：将一个数据集中的样本划分为若干个不相交的子集，每个子集称为一个“簇” (Cluster)。聚类的目标是使得**同一个簇内的样本彼此相似**，而**不同簇的样本彼此不相似**。这正是中文里“物以类聚，人以群分”思想的体现。
* **无监督特性**：与分类不同，聚类的数据没有预先定义的标签。算法需要自主地发现数据中潜在的结构和群组。因此，聚类的结果好坏通常没有绝对的标准，评估也更复杂。

---

### 6.2 万物皆有“距”：相似性的度量

如何定义“相似”？这是聚类的根本前提。通常我们通过**距离 (Distance)** 或 **相似度 (Similarity)** 来衡量。距离越小，相似度越高。

#### 6.2.1 对数值型数据 (Continuous Features)
假设有两个n维样本点 $x = (x_1, ..., x_n)$ 和 $y = (y_1, ..., y_n)$：

* **欧几里得距离 (Euclidean Distance, L2范数)**：空间中两点的直线距离，最常用。
    $$
    d_{euc}(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}
    $$
* **曼哈顿距离 (Manhattan Distance, L1范数)**：城市街区距离，即各坐标轴距离之和。对异常值没有欧氏距离敏感。
    $$
    d_{man}(x, y) = \sum_{i=1}^{n}|x_i - y_i|
    $$
* **余弦相似度 (Cosine Similarity)**：衡量两个向量方向的差异，与绝对大小无关。在文本分析等高维稀疏场景下尤其常用。相似度范围为[-1, 1]，通常转换为距离 `1 - similarity`。
    $$
    \text{sim}(x, y) = \cos(\theta) = \frac{x \cdot y}{\|x\| \|y\|}
    $$

#### 6.2.2 对类别型数据 (Categorical Features)

* **杰卡德相似系数 (Jaccard Similarity)**：用于比较两个集合的相似性，定义为交集大小除以并集大小。
    $$
    J(A, B) = \frac{|A \cap B|}{|A \cup B|}
    $$
    对应的杰卡德距离为 $d_J = 1 - J(A, B)$。

**注意**：在使用基于距离的算法前，通常需要对数据进行**标准化 (Standardization)**，以消除不同特征尺度（量纲）的影响。

---

### 6.3 “聚”得好不好：评估指标

聚类效果的评估分为两种情况：

#### 6.3.1 外部指标 (External Metrics) - 当存在真实标签时
这通常用于学术研究或算法评估，通过比较聚类结果与真实类别的一致性来衡量。

* **调整兰德系数 (Adjusted Rand Index, ARI)**：衡量两组数据划分的相似度，取值范围为[-1, 1]。值越大越好，接近0表示随机分配。
* **标准化互信息 (Normalized Mutual Information, NMI)**：从信息论角度衡量划分的相似度，取值范围[0, 1]，值越大越好。

#### 6.3.2 内部指标 (Internal Metrics) - 当不存在真实标签时
这是实际应用中最常用的评估方法，它仅利用聚类结果和数据自身的信息。

* **轮廓系数 (Silhouette Coefficient)**：衡量一个样本与其自身簇的紧密程度，以及与其他簇的分离程度。
    * 对样本 $i$，计算它到同簇其他样本的平均距离 $a_i$，以及它到**最近的**其他簇所有样本的平均距离 $b_i$。
    * 样本 $i$ 的轮廓系数为：$s_i = \frac{b_i - a_i}{\max(a_i, b_i)}$
    * 全体样本的轮廓系数是所有 $s_i$ 的平均值。取值范围[-1, 1]，值越大越好。1表示完美聚类，0表示簇重叠，-1表示聚类错误。
* **Calinski-Harabasz Index (CH分数)**：通过簇间离散度与簇内离散度的比值来评估，分数越高越好。
* **Davies-Bouldin Index (DBI)**：计算任意两簇的簇内距离之和与簇心距离的比值，寻找最小的均值。DBI越小越好。

---

### 6.4 主流聚类算法家族

#### 6.4.1 基于划分的聚类 (Partition-based Clustering)
**核心思想**：试图将数据集划分为K个不相交的簇，并通过迭代优化某个标准（如最小化簇内误差平方和）来寻找最佳划分。

##### K-Means (K-均值)
* **算法流程**:
    1.  **初始化**: 随机选择K个数据点作为初始质心 (Centroids)。(改进版K-Means++使用更智能的初始化方法)
    2.  **分配 (Assignment)**: 计算每个数据点到K个质心的距离，并将其分配给最近的质心所在的簇。
    3.  **更新 (Update)**: 重新计算每个簇的质心，即簇内所有数据点的均值。
    4.  **迭代**: 重复步骤2和3，直到质心位置不再变化或变化很小，或者达到最大迭代次数。
* **如何选择K值**:
    * **肘部法则 (Elbow Method)**: 绘制不同K值对应的**簇内平方和 (WCSS)** 曲线，选择曲线斜率变化最明显的“肘部”对应的K值。
    * **轮廓系数法**: 计算不同K值对应的平均轮廓系数，选择使得分最高的K值。
* **优点**:
    * 算法简单、快速，易于理解和实现。
    * 对于处理大数据集，算法是相对可伸缩的。
* **缺点**:
    * **必须预先指定K值**。
    * 对初始质心的选择敏感，可能陷入局部最优（K-Means++可缓解）。
    * 对异常值和噪声敏感。
    * 只能发现**球状**或凸形的簇，且假设簇的大小相当。

#### 6.4.2 基于层次的聚类 (Hierarchical Clustering)
**核心思想**：创建一套嵌套的聚类，最终形成一个树状的聚类结构，称为**树状图 (Dendrogram)**。用户可以根据需要在这个树的任意高度“横切一刀”，从而得到指定数量的簇。

* **主要类型**:
    * **凝聚型 (Agglomerative)**: 自底向上。开始时每个样本自成一簇，然后迭代地合并最相似的两簇，直到所有样本合并成一个簇。
    * **分裂型 (Divisive)**: 自顶向下。开始时所有样本在一个簇，然后迭代地分裂最不相似的簇，直到每个样本自成一簇。
* **算法流程**（以凝聚型为例）：
    1.  **初始化**：将数据集中的每一个样本点都视为一个独立的簇。如果有个 $n$ 个样本，初始就有 $n$ 个簇。
    2.  **计算初始距离**：计算每两个样本点之间的距离，形成一个距离矩阵。
    3.  **迭代合并**:
        a.  从距离矩阵中找到距离最近的两个簇（记为簇 $C_i$ 和 $C_j$）。
        b.  将这两个簇合并成一个新的簇 $C_{new} = C_i \cup C_j$。
        c.  **更新距离矩阵**：从矩阵中删除原来 $C_i$ 和 $C_j$ 的行和列，并添加新簇 $C_{new}$ 的行和列。新簇与其他簇 $C_k$ 的距离需要根据**链接准则 (Linkage Criteria)** 来计算。
        d.  重复步骤 a, b, c，直到所有样本点被合并到同一个簇中。
* **簇间距离/链接准则 (Linkage Criteria)**:
    * **Single-linkage**: 两簇间**最近**样本的距离。
    * **Complete-linkage**: 两簇间**最远**样本的距离。
    * **Average-linkage**: 两簇间所有样本对距离的**平均值**。
    * **Ward's method**: 合并后使得总簇内平方和增量最小的两簇。
* **优点**:
    * **无需预先指定K值**，可以根据树状图决定最终的簇数量。
    * 可以发现任意形状的簇（取决于链接准则）。
    * 提供了丰富的层次结构信息。
* **缺点**:
    * 计算复杂度高（通常为 $O(n^2\log n)$ 或更高），不适合大数据集。
    * 合并或分裂一旦做出，就无法撤销，可能导致次优解。

#### 6.4.3 基于密度的聚类 (Density-based Clustering)
**核心思想**：将簇定义为被低密度区域分隔开的稠密样本区域。

##### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
* **核心概念**:
    * `eps` (ε): 定义一个点的邻域半径。
    * `min_samples`: 成为核心点所需的邻域内最小样本数（包括自身）。
    * **核心点 (Core Point)**: 在其 `eps` 邻域内至少有 `min_samples` 个点的样本。
    * **边界点 (Border Point)**: 在某个核心点的 `eps` 邻域内，但自身不是核心点的样本。
    * **噪声点 (Noise Point)**: 既非核心点也非边界点的样本。
* **算法流程**:
    1.  从任意未访问的样本点P开始。
    2.  检查P是否为核心点。
    3.  **如果是核心点**: 创建一个新簇，并将P及其邻域内所有点（及这些点可密度直达的所有点）都加入该簇。
    4.  **如果不是核心点**: 暂时标记为噪声点（后续可能被其他核心点收编为边界点）。
    5.  重复以上过程，直到所有点都被访问。
* **优点**:
    * **无需预先指定K值**。
    * 能够发现**任意形状**的簇。
    * 对**异常值和噪声不敏感**，能将它们识别出来。
* **缺点**:
    * 对于密度不均匀的数据集效果不佳。
    * 对于高维数据，密度定义困难（维度灾难）。
    * 对参数 `eps` 和 `min_samples` 的选择敏感。

#### 6.4.4 谱聚类 (Spectral Clustering)

谱聚类是一种基于**图论**的聚类方法。它的核心思想不是直接在原始空间中对点进行分组，而是将聚类问题转化为图的分割问题，寻找最优的图切割方案。

* **核心概念**:

    想象一下，如果将所有数据点看作社交网络中的人，点与点之间的相似度看作他们之间的“友谊强度”。谱聚类的任务就是找到一种分群方式，使得“群组内部的朋友关系很密切”，而“群组之间的朋友关系很疏远”。

    它通过一种**“降维”**的方式，将数据映射到一个新的特征空间（谱空间），在这个新空间里，原本难以分离的簇变得更容易被K-Means等简单聚类算法分开了。

* **关键概念**

    * **相似性图 (Similarity Graph)**: 算法的第一步是将数据构建成一个图 $G=(V, E)$。
        * 顶点 $V$：每个数据点是一个顶点。
        * 边 $E$：连接两个顶点的边，其权重 $w_{ij}$ 表示点 $x_i$ 和 $x_j$ 的相似度。常用的相似度函数是高斯核函数（RBF核）：
            $$
            w_{ij} = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)
            $$
    * **邻接矩阵 (Adjacency Matrix, A)**: 图的权重矩阵， $A_{ij} = w_{ij}$。
    * **度矩阵 (Degree Matrix, D)**: 一个对角矩阵，对角线上的元素 $D_{ii}$ 是顶点 $i$ 的所有边的权重之和（即邻接矩阵第 $i$ 行的和）。
    * **拉普拉斯矩阵 (Laplacian Matrix, L)**: 谱聚类的核心。它反映了图的结构信息。最基本的形式是：
        $$
        L = D - A
        $$
        （也存在标准化的拉普拉斯矩阵，如 $L_{sym} = D^{-1/2}LD^{-1/2}$，通常效果更好）

* **算法流程**

    1.  **构建图**: 根据给定的数据集，选择一种方式构建相似性图，并计算其邻接矩阵 $A$ 和度矩阵 $D$。
    2.  **计算拉普拉斯矩阵**: $L = D - A$。
    3.  **计算特征值和特征向量**: 对拉普拉斯矩阵 $L$进行特征分解，求出其最小的 $k$ 个特征值及其对应的特征向量。
    4.  **构建新特征空间**: 将这 $k$ 个特征向量按列排成一个 $n \times k$ 的矩阵 $U$（$n$是样本数）。这个矩阵的每一行就是一个原始样本点在新的 $k$ 维空间中的表示。
    5.  **聚类**: 对这个新的 $n \times k$ 的数据矩阵 $U$，使用**K-Means**等简单的聚类算法，将其划分为 $k$ 个簇。最终的结果就是原始数据的聚类结果。

* **优点**:
    * **能够识别非凸形状的簇** (如月牙形、同心圆)，这是它相比K-Means最大的优势。
    * 只需要数据间的相似性矩阵，因此适用范围很广。
    * 对数据分布没有太多假设。

* **缺点**:
    * **计算开销大**：特征分解的计算复杂度通常为 $O(n^3)$，不适合非常大规模的数据集。
    * **需要预先指定簇数 K**。
    * 对相似性图的构建方式非常敏感，不同的图构建方法（如k-NN图、全连接图）和参数（如高斯核的 $\sigma$）对结果影响很大。

### 6.5 算法对比与选择

| 特性 | K-Means | 层次聚类 (凝聚型) | DBSCAN | 谱聚类 |
| :--- | :--- | :--- | :--- | :--- |
| **核心思想** | 最小化簇内平方和 | 合并最相似的簇 | 寻找密度相连的区域 | 降维 |
| **簇形状** | 球状/凸形 | 任意形状 | 任意形状 | 任意形状 |
| **需要指定K?** | **是** | 否 (但需指定切割高度) | 否 | 是 |
| **异常值处理** | 敏感，会将其分到某个簇 | 敏感，可能形成单独的小簇 | **鲁棒**，识别为噪声 | 敏感，会将其分到某个簇 |
| **计算复杂度** | 较快, $O(n \cdot K \cdot I \cdot d)$ | 较慢, $O(n^2\log n)$ | 中等, $O(n\log n)$ 或 $O(n^2)$ | 昂贵，$O(n^3)$ |
| **适用场景** | 簇为凸形且大小相似的大数据集 | 需要层次关系、数据集不大 | 簇形状不规则、含噪声的数据集 | 簇形状复杂、非凸，数据集不大 |

---

### 6.6 实践：Scikit-learn代码示例
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons, make_blobs
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, adjusted_rand_score

# --- 1. 生成和准备数据 ---
# 数据集1: 月亮形状，适合DBSCAN
X_moons, y_moons = make_moons(n_samples=200, noise=0.05, random_state=0)
X_moons = StandardScaler().fit_transform(X_moons)

# 数据集2: 球状，适合K-Means
X_blobs, y_blobs = make_blobs(n_samples=200, centers=3, n_features=2, random_state=0)
X_blobs = StandardScaler().fit_transform(X_blobs)

datasets = [(X_moons, y_moons), (X_blobs, y_blobs)]
dataset_names = ['Moons', 'Blobs']

# --- 2. 初始化聚类算法 ---
kmeans = KMeans(n_clusters=2, random_state=0, n_init='auto') # Moons数据只有2个簇
dbscan = DBSCAN(eps=0.3, min_samples=5)
agglomerative = AgglomerativeClustering(n_clusters=2, linkage='ward') # Moons数据只有2个簇

clfs = [kmeans, dbscan, agglomerative]
clf_names = ['K-Means', 'DBSCAN', 'Hierarchical']

# --- 3. 运行和评估 ---
plt.figure(figsize=(12, 6))
plot_num = 1

for i, (X, y) in enumerate(datasets):
    for name, clf in zip(clf_names, clfs):
        ax = plt.subplot(len(datasets), len(clfs), plot_num)
        
        # 适配blobs数据集的簇数
        if isinstance(clf, KMeans):
            clf.n_clusters = len(np.unique(y))
        if isinstance(clf, AgglomerativeClustering):
            clf.n_clusters = len(np.unique(y))
            
        # 拟合并预测
        clf.fit(X)
        y_pred = clf.labels_

        # 评估
        silhouette = silhouette_score(X, y_pred)
        ari = adjusted_rand_score(y, y_pred)

        # 绘图
        colors = np.array(['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3'])
        # 噪声点为黑色
        colors = np.append(colors, ['#000000'])
        ax.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])

        ax.set_title(f"{name} on {dataset_names[i]}", fontsize=9)
        ax.set_xticks(())
        ax.set_yticks(())
        ax.text(.99, .01, f'Sil: {silhouette:.2f}\nARI: {ari:.2f}',
                transform=ax.transAxes, size=9, horizontalalignment='right')
        plot_num += 1

plt.tight_layout()
plt.show()
```

## 7. 贝叶斯网络

### 7.1 基石：概率论的核心推论

贝叶斯网络完全建立在概率论的数学框架之上。理解以下几个核心概念是理解贝叶斯网络的前提。

#### 7.1.1 条件概率 (Conditional Probability)
一个事件A在另一个事件B已经发生的条件下发生的概率，记为 `P(A|B)`。
$$
P(A|B) = \frac{P(A, B)}{P(B)}
$$
其中 `P(A, B)` 是A和B的**联合概率 (Joint Probability)**，表示A和B同时发生的概率。

#### 7.1.2 乘法法则 (Product Rule)
由条件概率公式变形得到，用于计算联合概率：
$$
P(A, B) = P(A|B)P(B) = P(B|A)P(A)
$$

#### 7.1.3 链式法则 (Chain Rule)
乘法法则的推广，用于计算多个事件的联合概率。对于n个事件 $X_1, X_2, ..., X_n$：
$$
\begin{aligned}
P(X_1, ..., X_n) &= P(X_1)P(X_2|X_1)P(X_3|X_1, X_2)...P(X_n|X_1, ..., X_{n-1}) \\
&= \prod_{i=1}^{n} P(X_i | X_1, ..., X_{i-1})
\end{aligned}
$$
**核心痛点**：链式法则的计算复杂度是指数级的。当变量增多时，`P(X_n|X_1, ..., X_{n-1})` 这样的条件概率表会变得异常庞大，难以统计和存储。

#### 7.1.4 贝叶斯定理 (Bayes' Theorem)
结合乘法法则 `P(A, B)` 的两种写法，可以推导出贝叶斯定理，它是在已知结果（B）的情况下，反推原因（A）的概率。
$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$
* `P(A|B)`: **后验概率 (Posterior)**，结果B发生后，我们对原因A的信念更新。
* `P(B|A)`: **似然 (Likelihood)**，在原因A发生的条件下，结果B发生的概率。
* `P(A)`: **先验概率 (Prior)**，在没有任何证据前，我们对原因A的原始信念。
* `P(B)`: **证据 (Evidence)**，结果B发生的概率，通常通过全概率公式计算以作归一化。

---

### 7.2 核心定义：什么是贝叶斯网络？

贝叶斯网络 (Bayesian Network) 是一个**有向无环图 (Directed Acyclic Graph, DAG)**，它通过一种巧妙的方式，利用**条件独立性假设**来简化联合概率分布的计算。

它由两部分组成：

1.  **结构 (Structure)**: 一个DAG，其中：
    * **节点 (Nodes)**：代表随机变量 (Random Variables)。
    * **有向边 (Directed Edges)**：代表变量间的直接因果关系或依赖关系（从“父节点”指向“子节点”）。**无环**意味着不存在“A->B->C->A”这样的循环。

2.  **参数 (Parameters)**: 一系列**条件概率表 (Conditional Probability Tables, CPTs)**。
    * 每个节点 `X_i` 都关联一个CPT，这个CPT描述了该节点在其**父节点** `Parents(X_i)` 取不同值的组合下，自身的概率分布，即 `P(X_i | Parents(X_i))`。
    * 没有父节点的根节点，其CPT就是它的先验概率分布 `P(X_i)`。

**贝叶斯网络的核心贡献**：
它基于图的结构，假设**每个节点都与其非后代的节点条件独立于其父节点**。这个假设极大地简化了链式法则。

对于一个贝叶斯网络，n个变量的联合概率分布可以从复杂的链式法则：
$$
P(X_1, ..., X_n) = \prod_{i=1}^{n} P(X_i | X_1, ..., X_{i-1})
$$
简化为：
$$
P(X_1, ..., X_n) = \prod_{i=1}^{n} P(X_i | \text{Parents}(X_i))
$$
这个公式是贝叶斯网络的**灵魂**。它意味着我们不再需要存储指数级大小的条件概率表，只需要为每个节点存储一个与其父节点相关的、规模小得多的CPT。

---

### 7.3 灵魂：条件独立性与D-分离

如何从图结构中直接判断变量之间是否存在条件独立性？答案是**D-分离 (D-Separation)** 准则。

要判断变量集合X和Y是否在给定证据集合Z的条件下是独立的，我们需要检查X和Y之间在图上的**所有路径**。如果所有路径都被Z“阻断”(blocked)，那么X和Y就是条件独立的。

路径被阻断有三种情况：

1.  **串行连接 (Chain)**: `A → M → B`
    * 如果中间节点 `M` **被观测** (M在证据集Z中)，则路径被阻断。信息流在M处被截断。
    * 如果 `M` 未被观测，则路径是通的。

2.  **分叉连接 (Fork)**: `A ← M → B`
    * 如果共同的原因 `M` **被观测**，则路径被阻断。
    * 如果 `M` 未被观测，则路径是通的。

3.  **V型结构 (Collider / 对撞)**: `A → M ← B`
    * **默认情况**：如果对撞节点 `M` **及其任何后代都未被观测**，则路径**默认就是阻断的**。A和B的独立性被M保证。
    * **特殊情况**：如果 `M` **或者M的任何一个后代被观测**，则路径反而会**被打通**！这被称为“解释得通”效应（Explaining Away）。例如，地湿（M）的原因可能是下雨（A）或洒水（B）。如果我已经观测到地湿，那么得知下雨了，就会降低我对洒水车来过的信念。下雨和洒水这两个独立原因因为共同的结果产生了关联。

**总结**：只要 X 和 Y 之间的**每一条路径**都至少满足以上三种“阻断”情况中的一种，我们就称 X 和 Y 被 Z **D-分离**，即 `X ⊥ Y | Z`。

---

### 7.4 应用：贝叶斯网络能做什么？(推理任务)

有了网络之后，最核心的应用就是进行**概率推理 (Probabilistic Inference)**，即在获得部分信息（证据）后，更新对其他未知变量的信念。

1.  **概率查询 (Probability Queries)**: 给定一组证据变量 `E` 的观测值 `e`，计算一组查询变量 `Q` 的后验概率 `P(Q|E=e)`。这是最常见的任务。
    * 例如：在“草地湿了”(`E=e`)的条件下，“下雨了”(`Q`)的概率是多少？

2.  **最大后验概率 (MAP) 查询**: 也称为最可能的解释 (Most Probable Explanation, MPE)。给定证据 `E=e`，找到一组未观测变量 `W` 的最可能的状态组合 `w`。
    * `argmax_w P(W=w | E=e)`
    * 例如：已知“警报响了”，最可能发生的情况是“发生了地震但没有发生盗窃”还是“没有发生地震但发生了盗窃”？

---

### 7.5 算法：如何求解？(推理与学习)

#### 7.5.1 推理算法 (Inference Algorithms)
* **精确推理 (Exact Inference)**:
    * **变量消除法 (Variable Elimination)**: 通过动态规划的思想，利用乘法分配律，依次将非查询、非证据的变量通过求和（离散）或积分（连续）的方式“消除”掉，从而避免重复计算。对于稀疏连接的树状网络很高效，但对于复杂网络仍是NP-hard问题。
* **近似推理 (Approximate Inference)**: 当精确推理不可行时使用。
    * **随机抽样法**: 通过从网络中进行大量随机抽样来近似计算概率。代表算法有**MCMC (马尔可夫链蒙特卡洛)** 中的 **Gibbs抽样**。
    * **变分推断 (Variational Inference)**: 将计算后验概率的积分问题，转化为一个优化问题，寻找一个简单的、易于处理的分布 `q` 来近似真实的后验分布 `p`。

#### 7.5.2 学习算法 (Learning Algorithms)
如何从数据中构建贝叶斯网络？

* **参数学习 (Parameter Learning)**: **网络结构已知**，从数据中学习CPTs。
    * 数据完整时：本质上就是**最大似然估计 (MLE)**，即统计数据中的频率作为概率。
    * 数据有缺失时：需要使用 **EM (Expectation-Maximization) 算法** 或梯度下降等方法。
* **结构学习 (Structure Learning)**: **网络结构未知**，这是更困难的问题。
    * **基于分数的搜索**: 定义一个评分函数（如BIC、AIC分数），用于衡量网络结构与数据的匹配程度，然后在所有可能的图结构空间中搜索分数最高的结构。
    * **基于约束的方法**: 通过在数据上进行大量的条件独立性检验（如卡方检验），找到所有满足D-分离关系的变量对，然后构建出符合这些约束的图结构。

---

### 7.6 实战：手撕代码示例

我们用经典的“下雨-洒水-草地湿”例子来实现一个简单的贝叶斯网络，并进行推理。

**场景描述**:
* 草地湿 (G) 的原因可能是下雨 (R) 或洒水车工作 (S)。
* 下雨 (R) 会影响洒水车是否工作 (S)，假设天下雨时，洒水车有较大概率不工作。
* 这是一个V型结构: `R → S`, `R → G`, `S → G`。为了简化，我们用一个更经典的结构：`Cloudy -> Sprinkler`, `Cloudy -> Rain`, `Sprinkler -> GrassWet`, `Rain -> GrassWet`。这里有两个Fork和一个Collider。

**简化版问题：下雨(Rain)和洒水(Sprinkler)是独立的，但它们都会导致草地湿(GrassWet)。这是一个Collider结构。**
* `Rain → GrassWet`
* `Sprinkler → GrassWet`

**Python 实现**:

```python
import numpy as np

# 定义变量状态 (True/False)
T, F = True, False

# 1. 定义网络结构和CPTs
# 使用字典来表示CPTs
# P(Rain)
p_rain = {T: 0.2, F: 0.8}

# P(Sprinkler)
p_sprinkler = {T: 0.1, F: 0.9}

# P(GrassWet | Rain, Sprinkler)
p_grass_wet = {
    # Rain=T, Sprinkler=T
    (T, T): {T: 0.99, F: 0.01},
    # Rain=T, Sprinkler=F
    (T, F): {T: 0.8, F: 0.2},
    # Rain=F, Sprinkler=T
    (F, T): {T: 0.9, F: 0.1},
    # Rain=F, Sprinkler=F
    (F, F): {T: 0.0, F: 1.0},
}

# 2. 实现联合概率计算函数
# P(R, S, G) = P(R) * P(S) * P(G | R, S)
# 注意：这里我们假设R和S是独立的，符合Collider结构
def joint_probability(r, s, g):
    """计算给定状态组合的联合概率"""
    p_r = p_rain[r]
    p_s = p_sprinkler[s]
    p_g = p_grass_wet[(r, s)][g]
    return p_r * p_s * p_g

# 3. 进行推理：计算 P(Rain=T | GrassWet=T)
# 根据贝叶斯公式: P(R=T|G=T) = P(R=T, G=T) / P(G=T)

# 3.1 计算分子 P(R=T, G=T)
# 需要对Sprinkler变量进行边缘化 (sum out)
# P(R=T, G=T) = P(R=T, S=T, G=T) + P(R=T, S=F, G=T)
p_r_true_g_true = joint_probability(T, T, T) + joint_probability(T, F, T)

print(f"P(Rain=T, GrassWet=T) = {p_r_true_g_true:.4f}")

# 3.2 计算分母 P(G=T)
# 需要对Rain和Sprinkler两个变量进行边缘化
# P(G=T) = sum_{r,s} P(R=r, S=s, G=T)
p_g_true = 0.0
for r_val in [T, F]:
    for s_val in [T, F]:
        p_g_true += joint_probability(r_val, s_val, T)

print(f"P(GrassWet=T) = {p_g_true:.4f}")


# 3.3 计算最终结果
p_r_true_given_g_true = p_r_true_g_true / p_g_true

print(f"P(Rain=T | GrassWet=T) = {p_r_true_given_g_true:.4f}")

# 探索"解释得通"效应
# 计算在知道草地湿的情况下，洒水车也开了的条件下，下雨的概率
# P(Rain=T | GrassWet=T, Sprinkler=T) = P(R=T, G=T, S=T) / P(G=T, S=T)

# 分子
p_r_t_g_t_s_t = joint_probability(T, T, T)

# 分母 P(G=T, S=T) = P(R=T, G=T, S=T) + P(R=F, G=T, S=T)
p_g_t_s_t = joint_probability(T, T, T) + joint_probability(F, T, T)

p_r_t_given_g_t_s_t = p_r_t_g_t_s_t / p_g_t_s_t

print("--- Explaining Away Effect ---")
print(f"Original belief in rain P(Rain=T): {p_rain[T]}")
print(f"After seeing grass is wet P(Rain=T | GrassWet=T): {p_r_true_given_g_true:.4f}")
print(f"After also knowing sprinkler is on P(Rain=T | GrassWet=T, Sprinkler=T): {p_r_t_given_g_t_s_t:.4f}")
# 观察到概率下降，因为洒水车解释了草地湿的原因，降低了对下雨这个原因的信念。
```

## NLP/大模型

### .1 文本到向量：模型的“预处理器”

在CV中，我们直接处理像素矩阵；在NLP中，原始文本需要经过“分词”和“嵌入”两个关键步骤才能输入到Transformer模型中。

#### .1.1 文本分词 (Tokenization) - 定义模型的“单词”

这是将文本字符串转换为Token序列的过程。选择不同的分词策略，会直接影响模型的性能和效率。

* **词级分词** (Word-Level Tokenization)

    这是最符合人类直觉的分词方式，将完整的“单词”作为最小单元。

    * **核心思想**：语言的基本单位是词。
    * **具体方法**：
        * **空格分词 (Whitespace Tokenizer)**：这是最简单粗暴的方法。直接使用空格、换行符、制表符等空白字符作为分隔符来切分句子。例如，`"I am a student."` 会被切分为 `["I", "am", "a", "student."]`。这种方法无法处理标点符号，并且不适用于中文、日文等天然没有空格的语言。
        * **基于正则表达式的分词 (Regex-based Tokenizer)**：通过一个正则表达式来定义“什么是一个词”。例如，可以用 `\w+` 来匹配所有字母数字组合。这比简单的空格分词更灵活，可以更好地处理标点和特殊符号，但规则需要人为精心设计。
        * **基于词典的分词 (Dictionary-based Tokenization)**：在中文等场景下常见。通过维护一个巨大的词典，采用最大匹配等算法来切分句子。例如，“北京天安门”通过正向最大匹配和词典 `{"北京", "天安门"}` 可以被正确切分。
        
* **字符级分词** (Character-Level Tokenization)

    这是最细粒度的分词方式，将每个“字符”作为最小单元。

    * **核心思想**：语言的最小单位是字符。
    * **具体方法**：直接将字符串打散成字符列表。例如，`"hello"` 会被切分为 `['h', 'e', 'l', 'l', 'o']`。
    * **应用场景**：通常用于特殊任务，如拼写纠错，或者在处理非常罕见的词汇和噪声文本时作为一种兜底策略。在主流NLP任务中已不常用。

* **子词分词 (Subword Tokenization)**
    
    当前绝对的主流。
    
    * **核心思想**：在“完整单词”（Word-level）和“单个字符”（Character-level）之间取一个平衡。常见词作为一个整体，罕见词拆分为多个有意义的子词片段。
    * **优点**：
        1.  **无OOV (Out-of-Vocabulary)问题**：任何新词都可以被拆解成已知的子词单元。
        2.  **词表大小可控**：可以预设一个合理的词汇表规模（例如5万）。
        3.  **处理词形变化**：`running` 和 `run` 可以共享子词 `run`，模型能更好地学习它们的关系。
    * **主流算法**：
        * **BPE (Byte Pair Encoding)**：GPT系列使用。通过不断合并最高频的相邻Token对来构建词表。
        * **WordPiece**：BERT系列使用。与BPE类似，但合并的标准是最大化语言模型的“似然概率”。
        * **Unigram**：T5/XLNet使用。从一个非常大的词汇表开始，逐步剔除对整体损失影响最小的子词，直到达到目标大小。

#### .1.2 词嵌入 (Embeddings) - 将Token映射到高维空间

一旦文本被分词，每个Token就需要被转换成一个固定维度的向量。

* **经典静态嵌入**：
    * **Word2Vec / GloVe**：这些是“静态”的，即一个词（如`bank`）在任何上下文中的向量表示都是固定的。它们是通过在大型语料库上进行预训练得到的。现代模型已经很少直接使用它们了。
* **现代动态/上下文嵌入**：
    * **Transformer的输入层**：Transformer模型自身就包含一个Embedding层。这个层在训练开始时随机初始化，并在模型训练过程中端到端地学习。
    * **核心区别**：同一个词 `bank` 在 "river bank" 和 "investment bank" 这两个不同句子中，经过Transformer的多层处理后，其最终的向量表示是**完全不同**的。这解决了“一词多义”问题，是现代NLP模型性能飞跃的关键。

### .2 Transformer架构的“三种形态”

虽然都使用Transformer Block，但根据Block的组合方式，形成了三种主流架构，适用于不同类型的任务。

#### .2.1 Encoder-Only 架构 (仅编码器) - 擅长“理解”

* **代表模型**：**BERT**, RoBERTa
* **结构特点**：堆叠Transformer的Encoder层。其核心是**双向自注意力 (Bi-directional Self-Attention)**，即在处理一个Token时，它可以同时“看到”其左边和右边的所有上下文。
* **适用任务**：需要深度理解整个句子上下文的任务。
    * **文本分类**：情感分析、新闻分类。
    * **命名实体识别 (NER)**：找出文本中的人名、地名。
    * **句子关系判断**：判断两个句子是矛盾、蕴含还是中立。

#### .2.2 Decoder-Only 架构 (仅解码器) - 擅长“生成”

* **代表模型**：**GPT系列** (GPT-3, ChatGPT), LLaMA
* **结构特点**：堆叠Transformer的Decoder层。其核心是**单向/因果自注意力 (Causal/Masked Self-Attention)**，即在预测第 `i` 个Token时，它**只能**看到它前面的 `i-1` 个Token，不能看到未来的信息。
* **适用任务**：所有需要“依次生成”文本的任务。
    * **文本生成**：写文章、写代码。
    * **对话系统**：聊天机器人。
    * **文本补全**。

#### .2.3 Encoder-Decoder 架构 - 擅长“转换”

* **代表模型**：**T5**, BART, 原始的Transformer论文模型
* **结构特点**：一个Encoder处理输入序列，一个Decoder生成输出序列。Encoder和Decoder之间通过**交叉注意力 (Cross-Attention)**机制连接，Decoder在生成时会关注Encoder的输出。
* **适用任务**：输入和输出序列结构不同或语言不同的序列到序列（Seq2Seq）任务。
    * **机器翻译**：将德语翻译成英语。
    * **文本摘要**：输入一篇长文章，输出一个短摘要。
    * **问答系统**。

### .3 模型的“修炼内功”：训练范式

#### .3.1 预训练目标 (Pre-training Objectives)

模型如何在海量的无标签数据上学习？答案是“自监督学习”，即从数据自身创造标签。

* **BERT (Encoder) 的目标**：
    * **Masked Language Model (MLM)**：随机遮盖（Mask）掉输入句子中15%的Token，然后让模型去预测这些被遮盖的Token是什么。这迫使模型去学习双向的语境信息。
    * **Next Sentence Prediction (NSP)**：给模型两个句子A和B，让它判断B是不是A的真实下一句。这个目标后来被证明效果不大，在RoBERTa等后续模型中被放弃。
* **GPT (Decoder) 的目标**：
    * **Causal Language Model (CLM)**：非常简单直接，就是预测下一个词（Next Token Prediction）。给定一个句子的前 `n` 个词，预测第 `n+1` 个词。

#### .3.2 预训练与微调 (Pre-training & Fine-tuning)

这是BERT时代确立的经典范式，与CV领域完全一致。

1.  **预训练**：在通用、海量的文本数据上（如维基百科、书籍），使用MLM或CLM等自监督目标进行训练，让模型学习通用的语言知识。这个过程成本极高。
2.  **微调**：针对特定的下游任务（如情感分类），在小规模的、有标签的数据集上继续训练模型。通常只训练很少的轮次，调整模型的权重以适应特定任务。

#### .3.3 新兴的交互/训练范式

随着模型规模达到千亿级别，新的使用和训练方式变得流行。

* **Prompting (提示)**：不再微调模型参数，而是通过设计精巧的输入文本（Prompt）来“引导”或“激发”大模型直接完成任务。
    * **Zero-shot**：不给任何示例，直接下达指令。
    * **Few-shot**：在指令中给出几个示例，让模型依葫芦画瓢。
* **指令微调 (Instruction Tuning)**：收集大量“指令-回答”格式的数据对，对预训练好的大模型进行微调。这使得模型能更好地理解和遵循人类的指令，是ChatGPT类应用的关键技术。
* **RLHF (Reinforcement Learning from Human Feedback)**：人类反馈的强化学习。通过人类对模型生成的多个答案进行偏好排序，训练一个“奖励模型”，然后用强化学习算法根据这个奖励模型来进一步优化语言模型，使其输出更符合人类的期望（更有用、更无害、更诚实）。这是“对齐”（Alignment）技术的关键。

### .4 核心挑战与前沿方向

* **幻觉 (Hallucination)**：模型会“一本正经地胡说八道”，捏造事实、来源和数据。
* **对齐 (Alignment)**：如何让模型的价值观和行为与人类的价值观对齐，确保其安全、可靠和公平。
* **效率 (Efficiency)**：训练和推理的计算成本和能源消耗巨大。模型压缩（量化、剪枝、蒸馏）和高效推理是研究热点。
* **多模态 (Multi-modality)**：将文本、图像、声音等多种信息融合到单一模型中，是当前的一大趋势（例如CLIP, GPT-4V）。作为CV方向的同学，这是你的优势切入点。