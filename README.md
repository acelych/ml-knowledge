# 机器学习基础知识

## 1. 数值计算

数值计算的核心目标是：对于那些难以或无法求得精确解的数学问题，设计出高效的算法来寻找其**近似解**。

### 1.1 求解非线性方程 f(x) = 0

目标是找到方程的根 `x`。

#### 直接迭代法/不动点迭代法 (Fixed-Point Iteration)

任何非线性方程 `f(x) = 0`，我们都可以通过代数变形，将其转换为等价的**不动点方程**：
$$x = g(x)$$
一个满足 `x* = g(x*)` 的点 `x*`，就称为函数 `g(x)` 的一个**不动点 (Fixed Point)**。显然，`g(x)` 的不动点就是原方程 `f(x)=0` 的解。

直接迭代法，就是基于这个形式构造一个迭代序列：
1.  任选一个初始猜测值 `x₀`。
2.  反复使用迭代公式进行计算：
    $$x_{k+1} = g(x_k)$$
3.  如果这个序列收敛，其极限 `x*` 就是我们要求的解。

牛顿法是我们遇到的第一个特例。对于方程 `f(x)=0`，牛顿法构造的迭代函数是：
$$g(x) = x - \frac{f(x)}{f'(x)}$$
它是一种形式非常特殊且高效的不动点迭代。

#### 收敛定理（一阶导数判别法）

这个几何直观可以被严格地表述为以下定理：

> 如果不动点 `x*` 存在于某个区间 `I` 内，并且函数 `g(x)` 在该区间上满足以下两个条件：
> 1.  对任意 `x ∈ I`，都有 `g(x) ∈ I`。（迭代过程不会“跑出”这个区间）
> 2.  存在一个常数 `L < 1`，使得对所有 `x ∈ I`，都满足 $|g'(x)| \le L$。（`g(x)` 是一个**压缩映射 (Contraction Mapping)**）
>
> 那么，对于任意初始点 `x₀ ∈ I`，不动点迭代 `xₖ₊₁ = g(xₖ)` **必然收敛**到唯一的不动点 `x*`。

**应试核心结论**:
> **迭代收敛的一个充分条件是，在不动点 `x*` 附近，`|g'(x*)| < 1`。**
> - `|g'(x*)|` 的值越小，收敛速度越快。
> - 如果 `|g'(x*)| > 1`，迭代几乎必然发散。
> - 如果 `|g'(x*)| = 1`，收敛性不确定，可能收敛也可能发散。

**压缩映射** (Contraction Mapping)
- **思想**: 如果一个函数 ϕ(x) 能把一个区间内的任意两点的距离“压缩”得更近，那么它就是这个区间上的一个压缩映射。
- **直观理解**: 如果每次迭代都能让你离真解 x* 更近一点，那么这个过程最终必然会收敛。

$|x_{k+1}-x^*|=|g(x_k)-g(x^*)|<|x_k-x^*|$

要满足这个“压缩”条件，就需要函数 ϕ(x) 的变化率足够小。

#### 牛顿迭代法 (Newton's Method)
- **思想**: 在当前点 `xₖ` 做一条切线，用切线与x轴的交点作为下一次的近似解 `xₖ₊₁`。
- **公式**:
  $$x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}$$
- **特点**:
    - **优点**: 在根附近收敛速度极快（二次收敛）。
    - **缺点**: 需要计算导数 `f'(x)`；初始猜测值 `x₀` 需要离真解足够近，否则可能不收敛。

牛顿法的迭代函数 $g(x) = x - \frac{f(x)}{f'(x)}$。
我们对它求导，可以得到：
$$g'(x) = \frac{f(x)f''(x)}{[f'(x)]^2}$$
在不动点（也就是真解）`x*` 处，`f(x*) = 0`，所以：
$$g'(x^*) = 0$$
因为 `0 < 1`，这完美地满足了收敛条件！`g'(x*)` 等于0是它能够实现二次快速收敛的根本原因。这也解释了为什么牛顿法是如此高效和著名。

#### 二分法 (Bisection Method)
- **思想**: 保证根在一个区间 `[a, b]` 内 (即 `f(a)` 和 `f(b)` 异号)，然后不断将区间对半分割，每次都保留包含根的那一半区间。
- **步骤**:
    1. 找到 `a, b` 使得 `f(a)·f(b) < 0`。
    2. 计算中点 `c = (a+b)/2`。
    3. 如果 `f(a)·f(c) < 0`，则根在 `[a, c]` 内，令 `b=c`。否则根在 `[c, b]` 内，令 `a=c`。
    4. 重复2-3步，直到区间足够小。
- **特点**:
    - **优点**: 算法简单，只要初始区间有根，就一定能收敛。
    - **缺点**: 收敛速度慢（线性收敛）。

### 1.2 求解线性方程组 Ax = b (迭代法)

适用于大型稀疏矩阵，直接求解（如求逆）计算量过大。

#### 雅可比迭代法 (Jacobi Method)

雅可比迭代法是一种用于求解线性方程组 $Ax=b$ 的基础算法。其核心思想是：从一个初始猜测解出发，通过反复迭代，不断用上一轮的解来更新每一个变量，直到解的精度满足要求为止。它特别适用于系数矩阵 `A` 是**大型稀疏矩阵**的场景。

假设我们有如下 n 元线性方程组：
$$
\begin{cases}
a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n = b_1 \\
a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n = b_2 \\
\vdots \\
a_{n1}x_1 + a_{n2}x_2 + \dots + a_{nn}x_n = b_n
\end{cases}
$$

Step 1: 构造迭代公式
将第 `i` 个方程变形，用其他变量来表示 $x_i$：
$$x_i = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j \right)$$

将其写成迭代形式，用第 `k` 轮的解 $(x_1^{(k)}, x_2^{(k)}, \dots)$ 计算第 `k+1` 轮的解：
$$x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)$$

Step 2: 执行迭代
1.  选择一个初始解向量 $x^{(0)}$，通常是零向量。
2.  将 $x^{(0)}$ 代入迭代公式，计算出 $x^{(1)}$。
3.  将 $x^{(1)}$ 代入迭代公式，计算出 $x^{(2)}$。
4.  ...
5.  重复此过程，直到相邻两次迭代结果的差值足够小（例如 $|x^{(k+1)} - x^{(k)}| < \epsilon$），或者达到预设的最大迭代次数。

**收敛性判断**

雅可比法**不保证**对所有矩阵都收敛。

* **充分条件（易于判断）**: 系数矩阵 `A` 是**严格对角占优 (Strictly Diagonally Dominant)**。
    * **定义**: 矩阵每一行，对角元素的绝对值 **大于** 该行所有非对角元素的绝对值之和。
    * **公式**: 对于所有 $i$, 都有 $|a_{ii}| > \sum_{j \neq i} |a_{ij}|$
    * 只要满足此条件，迭代过程**必定收敛**。

* **充要条件（理论核心）**: 迭代矩阵 $T_J = -D^{-1}(L+U)$ 的**谱半径** $\rho(T_J) < 1$。
    * 其中 `D` 是 `A` 的对角部分，`L` 是 `A` 的严格下三角部分，`U` 是 `A` 的严格上三角部分。

**示例（考试原题）**

方程组:
$$
\begin{cases}
4x + y = 6 \\
x + 3y = 3
\end{cases}
$$

迭代公式:
$$
x_{k+1} = \frac{6 - y_k}{4}
$$
$$
y_{k+1} = \frac{3 - x_k}{3}
$$

第一次迭代:
* 初始解: $(x_0, y_0) = (0, 0)$
* 计算:
    * $x_1 = (6 - 0) / 4 = 1.5$
    * $y_1 = (3 - 0) / 3 = 1$
* 结果: $(x_1, y_1) = (1.5, 1)$

**优缺点**

* **优点**:
    * 算法逻辑简单，易于编程实现。
    * 每次迭代中，各个分量的计算是独立的，非常适合**并行计算**。
* **缺点**:
    * 收敛性不能保证。
    * 即使收敛，收敛速度通常也比高斯-赛德尔（Gauss-Seidel）等其他迭代法要慢。

总结如下：

- **思想**: 在第 `k+1` 次迭代中，计算所有新分量 `xᵢ⁽ᵏ⁺¹⁾` 时，**完全使用**第 `k` 次迭代的旧值 `xⱼ⁽ᵏ⁾`。
- **公式**:
  $$x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)$$
- **特点**:
    - **优点**: 各个分量的计算相互独立，非常容易并行化。
    - **缺点**: 收敛速度通常较慢，且收敛性要求比高斯-赛德尔法更苛刻。

#### 高斯-赛德尔迭代法 (Gauss-Seidel Method)
- **思想**: 在第 `k+1` 次迭代中，计算新分量 `xᵢ⁽ᵏ⁺¹⁾` 时，**立即使用**本轮次已经计算出的新值 `xⱼ⁽ᵏ⁺¹⁾` (j < i)。
- **公式**:
  $$x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right)$$
- **特点**:
    - **优点**: 通常比雅可比法收敛更快。
    - **缺点**: 计算是顺序依赖的，不易并行化。

### 1.3 插值与拟合

目标是根据已知的离散数据点，构造一个连续函数。

#### 1.3.1 拉格朗日插值 (Lagrange Interpolation)
- **思想**: 构造一组“基多项式” `Lᵢ(x)`，每个 `Lᵢ(x)` 在 `xᵢ` 处取值为1，在所有其他的 `xⱼ` 处取值为0。最终的插值多项式就是这些基多项式的加权和。
- **公式**:
  $$P(x) = \sum_{i=0}^{n} y_i L_i(x), \quad \text{其中 } L_i(x) = \prod_{j=0, j \neq i}^{n} \frac{x-x_j}{x_i-x_j}$$
- **特点**:
    - **优点**: 公式结构清晰，易于理解。
    - **缺点**: 计算量大；每增加一个数据点，所有基多项式都需要重新计算。

#### 1.3.2 牛顿插值法 (Newton's Interpolation)

此方法主要为了解决拉格朗日插值“不灵活”的问题。

- **核心思想**:
    采用一种“递推”或“增量”的形式来构造多项式。每增加一个新点，不是推倒重来，而是在原有基础上增加一个“修正项”。

- **核心工具：均差 (Divided Differences)**
    均差是牛顿插值的灵魂，它是一个递归定义的概念。
    - **零阶均差**: $f[x_i] = y_i$
    - **一阶均差**: $f[x_i, x_j] = \frac{f[x_j] - f[x_i]}{x_j - x_i}$
    - **二阶均差**: $f[x_i, x_j, x_k] = \frac{f[x_j, x_k] - f[x_i, x_j]}{x_k - x_i}$
    - **k阶均差**: $f[x_0, \dots, x_k] = \frac{f[x_1, \dots, x_k] - f[x_0, \dots, x_{k-1}]}{x_k - x_0}$
    在计算时，我们通常会列一个**均差表**来系统地算出所有需要的均差值。

- **牛顿插值公式**:
    $P(x) = f[x_0] + f[x_0, x_1](x-x_0) + f[x_0, x_1, x_2](x-x_0)(x-x_1) + \dots$
    
    可以看到，每多一项，都是在前一项的基础上进行补充。

- **特点**:
    - **优点**: 继承了拉格朗日插值的优点（是同一个多项式），但计算上更高效。特别是当需要**增加新的插值点**时，只需在公式末尾追加新的一项即可，非常灵活。
    - **缺点**: 仍然是单一的高次多项式，无法避免龙格现象。

#### 1.3.3 样条插值 (Spline Interpolation)

此方法主要为了解决高次多项式的“振荡”问题。

- **核心思想**:
    放弃使用一个单一的、贯穿所有点的高次多项式。取而代之的是，在每两个相邻的数据点之间，使用一个**低次多项式**（通常是三次，即Cubic Spline），然后通过添加约束条件，让这些分段的多项式在连接点（称为“节点”）处**平滑地拼接**起来。

- **三次样条 (Cubic Spline) 的“平滑”条件**:
    对于每一个内部节点 `xᵢ`，它左右两边的两个三次多项式必须满足：
    1.  **函数值相等 (C⁰连续)**: 保证曲线是连续的。
    2.  **一阶导数值相等 (C¹连续)**: 保证曲线在节点处的**斜率**是连续的，没有尖角。
    3.  **二阶导数值相等 (C²连续)**: 保证曲线在节点处的**曲率**是连续的，形态变化平滑。

- **边界条件**:
    为了唯一确定所有分段三次多项式的系数，还需要在两个端点（`x₀`和`xₙ`）处各补充一个条件。常见的有：
    - **自然样条 (Natural Spline)**: 令两个端点的二阶导数为0。
    - **钳制样条 (Clamped Spline)**: 指定两个端点的一阶导数值。

- **特点**:
    - **优点**: 解决了龙格现象！生成的曲线非常平滑、稳定，是工程和计算机图形学中最常用的插值方法。
    - **缺点**: 最终得到的函数是一个分段函数，而不是一个统一的表达式。计算过程需要求解一个线性方程组。

#### 1.3.4 埃尔米特插值 (Hermite Interpolation)

此方法是一种更高阶的插值，它不仅要求函数曲线穿过数据点，还要求在这些点的“姿态”（斜率）也完全一致。

- **核心思想**:
    构造一个多项式 `P(x)`，它不仅满足 $P(x_i) = y_i$，同时还满足**一阶导数值也相等**，$P'(x_i) = y'_i$。

- **要求**:
    输入的数据不仅包含数据点 `(xᵢ, yᵢ)`，还必须包含这些点的**一阶导数值 `y'ᵢ`**。

- **结果**:
    对于 `n+1` 个数据点，如果我们同时给定了它们的函数值和一阶导数值（共 `2n+2` 个条件），那么可以唯一确定一个次数不超过 `2n+1` 的埃尔米特插值多项式。

- **特点**:
    - **优点**: 由于利用了导数信息，插值函数与原始数据点的“贴合度”非常高，曲线在节点处的形态和趋势都更为精准。
    - **缺点**: **对数据的要求非常苛刻**。在实际问题中，我们往往只能观测到函数值 `yᵢ`，很难获得其精确的导数值 `y'ᵢ`。

### 1.4 数值积分

计算定积分 $\int_a^b f(x)dx$ 的近似值。

#### 梯形法则 (Trapezoidal Rule)
- **思想**: 将积分区间分割成若干小段，用一系列小梯形的面积之和来近似曲线下的总面积。
- **公式 (复合梯形)**:
  $$\int_a^b f(x)dx \approx \frac{h}{2} [f(x_0) + 2\sum_{i=1}^{n-1}f(x_i) + f(x_n)]$$
  其中 `h = (b-a)/n` 是步长。
- **特点**: 简单直观，误差阶为 $O(h^2)$。

#### 辛普森法则 (Simpson's Rule)
- **思想**: 将积分区间分割成若干小段，每**两段**用一条抛物线来拟合曲线，用抛物线下的面积来近似真实面积。
- **公式 (复合辛普森)**:
  $$\int_a^b f(x)dx \approx \frac{h}{3} [f(x_0) + 4\sum_{i=1, i \text{ is odd}}^{n-1}f(x_i) + 2\sum_{i=2, i \text{ is even}}^{n-2}f(x_i) + f(x_n)]$$
  其中 `n` 必须是偶数。
- **特点**: 比梯形法则精度高得多，误差阶为 $O(h^4)$。

### 1.5 常微分方程初值问题

求解形如 `y' = f(x, y)`, `y(x₀) = y₀` 的问题。

#### 欧拉法 (Euler's Method)
- **思想**: 在当前点 `(xₙ, yₙ)`，沿着由 `f(xₙ, yₙ)` 给出的切线方向，前进一小步 `h`，以此来估算下一个点 `yₙ₊₁` 的位置。
- **公式**:
  $$y_{n+1} = y_n + h \cdot f(x_n, y_n)$$
- **特点**:
    - **优点**: 极其简单，是所有更高级方法的基础。
    - **缺点**: 精度很低，误差会随步数累积，不稳定。

## 2. 线性代数

### 2.1 空间的基本构成：从向量到空间

#### 张成 (Span)
* **定义**: 一组向量的所有可能的线性组合构成的集合。
* **直观理解**: 给定一组基础向量（积木），它们能“搭建”出的整个几何空间（直线、平面、体）。
* **示例**: `span(α₁, α₂)` 构成一个二维平面。

#### 线性无关 (Linear Independence)
* **定义**: 一组向量中，没有任何一个向量可以被其他向量的线性组合所表示。
* **直观理解**: 每一块“积木”都提供了全新的、不可替代的方向。`α₁`, `α₂`, `α₃` 线性无关，意味着它们分别指向了三个完全不同的维度。

#### 基 (Basis)
* **定义**: 张成整个空间的一组**线性无关**的向量。
* **直观理解**: 搭建整个空间所需要的**最少数量**的“积木”。基向量的数量决定了空间的维度。
* **示例**: `{α₁, α₂, α₃}` 是三维空间的一组基。

#### 维度 (Dimension)
* **定义**: 构成空间的一组基中向量的数量。
* **直观理解**: 描述一个空间有多“大”，需要几个独立的坐标轴才能完全描述。

### 2.2 矩阵的核心功能：空间变换

一个 `m x n` 的矩阵 A，可以看作一个从 n 维空间到 m 维空间的线性变换。

#### 零空间 (Null Space / Kernel)
* **定义**: 所有被矩阵 A 变换为零向量的输入向量 `x` 的集合，即 $A\vec{x} = 0$ 的所有解。
* **直观理解**: 变换中所有被“压扁”到原点的信息。
* **基础解系**: 零空间的一组基。
* **零度 (Nullity)**: 零空间的维度，即基础解系中向量的数量。

#### 秩 (Rank)
* **定义**: 矩阵 A 的列向量张成的空间的维度。
* **直观理解**: 变换后输出空间（像空间）的维度。代表了变换保留了多少维度的信息。
* **计算**: 对矩阵进行高斯消元，得到的阶梯形矩阵中非零行的数量。

#### 秩-零度定理 (The Rank-Nullity Theorem)
* **公式**: `Rank(A) + Nullity(A) = n` (矩阵 A 的**列数**)
* **内在联系**: 输入空间的维度(`n`)，被划分为了两部分：一部分被有效变换（秩），另一部分则被压缩为零（零度）。能量守恒！

### 2.3 变换的深度解析：特征值与特征向量

这是理解一个变换**内在特性**的关键。

#### 特征值 (Eigenvalue, λ) & 特征向量 (Eigenvector, v)
* **定义**: 满足 $A\vec{v} = \lambda\vec{v}$ 的非零向量 $\vec{v}$ 和标量 $\lambda$。
* **直观理解**:
    * **特征向量**: 空间变换中的“主轴”或“不变的方向”。在变换中只发生伸缩，不发生旋转。
    * **特征值**: 在“主轴”方向上的伸缩比例。

#### 特征空间 (Eigenspace, Eλ)
* **定义**: 对应于同一个特征值 `λ` 的所有特征向量，以及零向量，所构成的子空间。
* **直观理解**: 一个“命运共同体”，在这个子空间内的所有向量，都被以同样的比例 `λ` 进行伸缩。它的维度被称为特征值 `λ` 的**几何重数**。
* **示例**: 第五题中，`E₀ = span(α₁, α₂)` 是一个二维特征空间；`E₂ = span(α₃)` 是一个一维特征空间。

### 2.4 可对角化 (Diagonalizability)

* **定义**: 一个 n x n 矩阵 A 如果有 n 个线性无关的特征向量，那么它就是可对角化的。
* **等价条件 (你的直觉)**: 所有特征空间的维度之和，恰好等于矩阵的维度 n。
    * `Σ dim(Eλ) = n`
* **内在联系**: 如果一个矩阵可对角化，意味着它的特征向量足以**张成**整个空间。我们可以切换到这组特征向量构成的“完美坐标系”（基），在这个坐标系下，复杂的变换 A 就变成了一个简单的、只有拉伸作用的对角矩阵 D。这是线性代数中一个极其优美的结论。
* **第五题的升华**: `dim(E₀) + dim(E₂) = 2 + 1 = 3`，等于空间维度。所以该问题中的矩阵是可对角化的。整个空间被特征空间“填满”，没有任何“多余的空位”。因此，任何跨越不同特征空间的向量组合（如D选项），必然不属于任何一个特征空间，故不是特征向量。

### 2.5 实对称矩阵 (Real Symmetric Matrix)

这是性质最丰富、在机器学习中最重要的矩阵类型之一。协方差矩阵就是一个典型的实对称矩阵。

- **定义**: 一个矩阵 `A` 如果满足 $A^T = A$，并且所有元素均为实数，则称其为实对称矩阵。

- **核心性质**:
    1.  **特征值均为实数**: 这是最基本也是最重要的性质。它保证了诸如主成分分析(PCA)中的“方差”这类物理量是真实可测的，而不是复数。
    2.  **不同特征值对应的特征向量相互正交**: 如果 $\lambda_1 \neq \lambda_2$，那么它们对应的特征向量 $\vec{v}_1, \vec{v}_2$ 满足 $\vec{v}_1^T \vec{v}_2 = 0$。
    3.  **可正交对角化**: 综合以上性质，对于任何一个实对称矩阵 `A`，我们一定可以找到一个**正交矩阵** `Q` (满足 $Q^{-1} = Q^T$)，使得：
        $$A = QDQ^T$$
        其中 `D` 是由 `A` 的特征值构成的对角矩阵，`Q` 的列向量是与这些特征值对应的、标准正交化的特征向量。这个过程也叫**谱分解 (Spectral Decomposition)**。

- **与机器学习的联系**:
    - **主成分分析 (PCA)**: PCA的本质就是对数据的协方差矩阵进行谱分解。特征值代表了数据在各个主成分方向上的方差，特征向量就是主成分的方向。

### 2.6 SVD 分解 (Singular Value Decomposition)

SVD是线性代数中的“瑞士军刀”，它强大到可以应用于**任何 `m x n` 的矩阵**，无论方阵、非方阵、满秩、亏秩。

- **定义**: 任何矩阵 `A` 都可以分解为：
    $$A_{m \times n} = U_{m \times m} \Sigma_{m \times n} V_{n \times n}^T$$

- **核心构成**:
    - **`U` (左奇异向量)**: 一个 `m x m` 的**正交矩阵**。其列向量是 $AA^T$ 的特征向量。在数据分析中，`U` 的列通常与“行”的某种主题或概念相关（例如，用户主题）。
    - **`Σ` (奇异值)**: 一个 `m x n` 的**对角矩阵**（准确说是伪对角），对角线上的元素 $\sigma_i$ 称为**奇异值**。它们非负，并按从大到小的顺序排列。奇异值是 $A^TA$ (或 $AA^T$) 的特征值的平方根。
    - **`V` (右奇异向量)**: 一个 `n x n` 的**正交矩阵**。其列向量是 $A^TA$ 的特征向量。在数据分析中，`V` 的列通常与“列”的某种主题或概念相关（例如，物品主题）。

- **重要性质与直观理解**:
    - **几何意义**: SVD将任意一个线性变换分解为三个基本动作：一个**旋转** ($V^T$)、一个沿坐标轴的**缩放** ($\Sigma$)、再加另一个**旋转** ($U$)。
    - **唯一性**: 对于任意矩阵，其**奇异值是唯一确定的**。但左右奇异向量不唯一（例如，对应的列向量同时乘以-1仍然成立）。这正是之前第9题的考点。
    - **低秩近似**: SVD最重要的应用。奇异值的大小代表了数据中对应“模式”的重要性。我们可以只保留最大的 `k` 个奇异值及其对应的奇异向量来近似原始矩阵，这在**降维、数据压缩、去噪**中是核心技术。

- **与机器学习的联系**:
    - **推荐系统**: 在用户-物品评分矩阵中，用SVD进行矩阵分解，发现用户和物品的潜在因子。
    - **自然语言处理**: 潜在语义索引 (LSI) 技术就是基于SVD。
    - **图像处理**: 图像压缩。

### 2.7 QR 分解 (QR Decomposition)

QR分解常用于数值计算，特别是求解线性方程组和特征值问题。

- **定义**: 任何满秩的 `m x n` 矩阵 `A` 都可以分解为：
    $$A = QR$$

- **核心构成**:
    - **`Q`**: 一个 `m x m` 的**正交矩阵**。
    - **`R`**: 一个 `m x n` 的**上三角矩阵**。

- **直观理解**:
    - QR分解的本质是**施密特正交化 (Gram-Schmidt Orthogonalization)** 的稳定实现。
    - 我们可以把矩阵 `A` 的列向量看作空间中的一组基。QR分解的过程就是把这组普通的基，变成一组**标准正交基**（`Q`的列向量），而 `R` 矩阵则记录了如何通过这组新的标准正交基来表示回原始的基。

- **与机器学习的联系**:
    - **最小二乘法求解**: 在解线性回归 $A\vec{x} = \vec{b}$ 时，如果 `A` 不是方阵，通常求解正规方程 $A^TA\vec{x} = A^T\vec{b}$。使用QR分解可以更稳定、更高效地求解。

### 2.8 LU 分解 (LU Decomposition)

LU分解主要用于求解线性方程组和求逆矩阵，它本质上是高斯消元法的矩阵形式。

- **定义**: 一个**方阵** `A` 如果满足一定条件，可以分解为：
    $$A = LU$$

- **核心构成**:
    - **`L`**: 一个**下三角矩阵 (Lower triangular)**，对角线元素为1。
    - **`U`**: 一个**上三角矩阵 (Upper triangular)**。

- **直观理解**:
    - **高斯消元法的“记账本”**。`U` 矩阵就是我们辛辛苦苦通过行变换进行高斯消元后得到的最终结果（阶梯形矩阵）。而 `L` 矩阵则像一个账本，记录了我们消元过程中所有的操作步骤（比如“第二行减去第一行的2倍”，这个“2”就会被记在`L`的对应位置）。

- **存在性**:
    - LU分解**并非对所有方阵都存在**。当高斯消元过程中出现主元（对角线元素）为0，导致无法继续时，就需要进行“行交换”。加入了行交换的分解称为 `PA = LU` 分解，其中 `P` 是一个记录了行交换操作的置换矩阵。

- **与机器学习的联系**:
    - **高效求解线性方程组**: 当需要对同一个矩阵 `A` 和多个不同的向量 `b` 求解 `Ax=b` 时，LU分解的优势巨大。我们只需对 `A` 分解一次，然后通过简单的向前和向后代入法（解 $L\vec{y}=\vec{b}$ 和 $U\vec{x}=\vec{y}$）即可快速求出解，避免了重复进行高斯消元或求逆的巨大计算量。

## 2# 线性代数几何直观

这份笔记旨在超越公式，构建一个关于线性代数核心思想的几何与物理直觉。

### 2#.1 矩阵的本质：空间的变换场

矩阵不仅是数字的网格，其本质是一个**线性变换 (Linear Transformation)**。我们可以将其想象成一个定义在整个空间中的**矢量场**，它规定了空间中每一个点 `x` 将如何“流动”到新的点 `Ax`。一个具象的可视化解释就是机器学习当中的前馈神经网络/多层感知机。

- **几何图像**: 矩阵的列向量，精准地揭示了标准坐标系的基向量（单位向量 `i, j, k`）变换后的新位置。整个空间的扭曲、缩放、旋转，都是由这几个基向量的“命运”所决定的。
- **维度升降**:
    - **满秩 (Full Rank)**: 矩阵的列向量线性无关。变换保留了空间的维度，仅仅是“扭曲”了它。
    - **亏秩 (Rank Deficient)**: 矩阵的列向量线性相关。说明向量组存在“重复”的基，即缺少支撑原始维度的基，变换会将原始空间“压扁”或“坍缩”到一个更低维的子空间（即矩阵的列空间/值域）中。那些被“压扁”到零点的向量，构成了矩阵的**零空间 (Null Space)**。

### 2#.2 变换的深度解析：寻找不变的“主轴”

在复杂的空间流动中，我们渴望找到其内在的稳定结构。**特征向量 (Eigenvectors)** 和 **特征值 (Eigenvalues)** 就是这个稳定结构的核心。线性变换可以拆解为两个部分：旋转/镜像与剪切，即对坐标系的变换，以及缩放，即“相似”概念中描述的“不同坐标系下的同一个变换”的变换。前者表现为特征向量/特征空间，后者表现为特征值。

- **特征向量**: 构成变换场中“主轴”或“不变方向”的基本单元。位于这些方向上的向量，在变换中只会被缩放，**不会发生方向的偏转（旋转/镜像或剪切）**。
- **特征值**: 对应主轴方向上的缩放比例，是线性变换抛开坐标系视角后的变换本身。
- **特征空间 (Eigenspace)**: 由同一特征值对应的所有特征向量构成的子空间，描述了“主轴”的完整实体。它是一个**不变子空间**，内部的任何向量在变换后，结果仍然留在这个子空间内。

### 2#.3 理想世界：可对角化的变换

当一个n维变换拥有n个线性无关的特征向量时，我们就处在一个“理想世界”中。这意味着它的特征空间足以**张成 (Span)** 整个n维空间。

- **相似**：特征值是一切线性变换的特征，这就是说拥有相同特征模式的矩阵实际上是在不同的坐标系视角下进行相同的变换，即满足 $P^{-1}AP=B$ 则说明 $A\sim B$。要强调的是，“特征模式相同”不仅要求特征值相同，特征值的几何重数也应该相同，几何重数在这里扮演着各个特征值对整个特征空间维度的分配与统治关系。

- **相似对角化 ($A = PDP^{-1}$)**: 这不是一个单纯的公式，而是一种**视角的切换**。
    - **`A`**: 变换在标准坐标系下的（通常是复杂的）描述。
    - **`P`**: 一个“坐标系翻译官”（由特征向量作列构成），负责将向量从“特征坐标系”翻译回“标准坐标系”。
    - **`D`**: 同一个变换，在“特征坐标系”下的（极其简单的）描述。它是一个对角矩阵，其特征空间基底是独热的，其元素就是特征值，这就是说，对角阵是一种没有旋转/镜像与剪切，仅缩放的线性变换，这完美地揭示了变换的本质——**纯粹的、沿着主轴的缩放**。

- **正交矩阵**：正交矩阵是一种特殊的可逆矩阵，它的向量组之间两两正交且模长为1，这种矩阵的实特征值的绝对值为1，这说明正交矩阵没有缩放，是纯粹的各向同性的旋转/镜像的表现。

- **正交对角化**：谱定理告诉我们，一个实矩阵 $A$ 可以被正交对角化的充要条件是：$A$ 是实对称矩阵。这就是说，实对称矩阵是一种在正交空间中进行缩放变换的线性变换，可以通过对应的正交阵进入这个正交空间。对于坐标系变换不满足各向同性要求的可对角化的矩阵，其 **`P`** 矩阵不可为正交矩阵。

### 2#.4 现实世界的复杂性：剪切现象

如果说纯粹缩放表现为对角阵，纯粹旋转/镜像（各向同性）表现为正交阵，那么对剪切操作的解释则相对复杂很多。当一个n维变换的线性无关特征向量不足n个时（即某特征值的**几何重数 < 代数重数**），理想世界被打破，复杂的**剪切 (Shear)** 现象登场。

#### 剪切的双重视角

对于剪切矩阵 $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$，我们可以从两个视角去理解：

- **视角一：原始矩阵 $A$ (全局/现象视角)**
    我们观察到的是变换的最终效果。在这里，我们发现了一个奇特的“**分量守恒**”现象：输入向量 `[x, y]` 变换为 `[x+y, y]`，其 **y分量的值保持不变**。这是剪切变换在全局坐标系下表现出的代数属性，这种对特定维度“不干涉”的现象是特征空间维数缺损在变换场域中的直接表现。

- **视角二：算子 $A - λI$ (诊断/机理视角)**
    这是深入变换内部的“诊断工具”。对于剪切矩阵，其唯一的特征值是 $λ=1$。我们考察 $A - I = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$。这个算子滤除了“1倍缩放”的良性行为，暴露了其“病灶”：它将y轴方向的向量 `[0, y]` “滑动”并“拍扁”到了x轴上 `[y, 0]`。这揭示了特征空间维数缺损的**内部动力学机制**的表现。

#### 广义特征空间与变换链条

“消失”的特征空间维度并没有蒸发，而是与仅存的特征空间纠缠在一起，形成了**广义特征空间 (Generalized Eigenspace)**。

- **变换链条**: 在剪切矩阵的广义特征空间内，向量不再是平等的。它们形成了一个“等级链条”。
    - **贵族 (普通特征向量 `v`)**: 位于x轴，满足 `(A-I)v = 0`，享受纯粹的1倍缩放。
    - **仆人 (广义特征向量 `u`)**: 例如y轴向量，它不再满足 `(A-I)u=0`，而是满足 `(A-I)u = v`。

- **剪切与特征值的绑定**:
    这个链条关系 `Au - u = v` 可以写为：
    $$Au = 1 \cdot u + v$$
    这个公式是理解剪切与特征值绑定的钥匙。它表明，对广义特征向量 `u` 的变换，由两部分构成：
    1.  一部分是**与其绑定的特征值 `λ=1` 所定义的缩放** (`1 * u`)。
    2.  另一部分是一个**额外的、沿着“贵族”特征向量 `v` 方向的“推移”**，这就是**剪切**。

剪切，本质上是一个重根特征值无法完全实现其所有维度的纯粹缩放，从而在内部产生的一种“补偿性”的、等级化的复杂变换。整个故事（缩放、滑动、纠缠）都发生在与这一个特征值`λ`相关联的广义特征空间内部，因此两者严格绑定。**约旦标准型 (Jordan Form)** 则是将矩阵写成这种“一个特征值，一个故事”的最终形式。

### 2#.5 奇异值分解（SVD）：线性变换的大一统模型

对于任何一个线性变换（无论它是否会改变空间的维度），我们能否找到一组‘最完美’的标准正交坐标系，使得变换在这组坐标系之间的作用，仅仅是纯粹的缩放？谱分解（特征值分解）给出了一个直观但不通用的答案，而奇异值分解则认为，对于任何矩阵，我们都能找到两个这样的坐标系：一个用于输入空间，一个用于输出空间。

特征值分解问的是一个向内的问题，它关心的是变换的内在结构和稳定性，特征向量就是这个结构中“不变”的核心。如果一个变换的内在结构本身就是“拧巴”的（比如剪切矩阵），它的某些特征空间维度被隐藏起来了，成为了广义特征空间，那么这种“内省”式的剖析就会失败，这就是不可对角化。

要从特征值走向奇异值，我们仅需转换一个视角：从追求 $Ax=\lambda x$ 中 $x$ 的前后一致性，到追求 $Av=\sigma u$ 中 $\sigma$ 可能的最大值。假设 $A^TAv=\lambda v$，则存在谱分解：$A^TA=V\Lambda V^T$，令 $\Sigma=\sqrt{\Lambda},\ U=AV\Sigma^T$，则 $U^TU=\Sigma V^TA^TAV\Sigma^T=\Sigma V^TV\Lambda V^TV\Sigma^T=I$，说明 $A=U\Sigma V^T$ 表示 $A$ 一定能分解为两个正交阵和一个对角阵的形式。

- **转置**：矩阵转置是一个比较初级的概念，不过要清楚地理解它的几何直观，我们不得不绕一大圈，从SVD的角度去分析。
对于任意矩阵我们有：$A=U\Sigma V^T$，假设它是可逆的，则它的逆矩阵则是：$A^{-1}=(V^T)^{-1}\Sigma^{-1}U^{-1}=V\Sigma^{-1}U^T$，其转置则是：$A^T=(V^T)^T\Sigma^TU^T=V\Sigma U^T$，也就是说，矩阵的转置和矩阵的逆的唯一差别，在于转置没有像反转旋转变换那样反转缩放变换，保持了与原矩阵相同的缩放变换。

- **奇异值分解**：理解奇异值分解的代数几何直观，最好的方式是从得证的结论出发，回溯性地分析它和特征值分解（谱分解）的差别。

    - **$A^TA$与$AA^T$**：对于任意可逆矩阵我们有：$A=U\Sigma V^T$，沿用转置讨论中得到的结论，那么 $A^TA=(V\Sigma U^T)(U\Sigma V^T)=V\Sigma^2V^T$，同理 $AA^T=U\Sigma^2 U^T$，就是说，$A^TA$ 和 $AA^T$ 的第一种解读是“中和掉了矩阵 $A$ 的旋转操作，但保留了两次缩放操作”；第二种解读是“只进行‘入场/出场’的正逆变换，抵达中转空间后缩放一下就回去了”。这说明了，$A^TA$ 和 $AA^T$ 一定是半正定（特征值是奇异值的平方）实对称矩阵（可以被正交对角化）。
    - **谱分解**：谱分解的核心观念是：“线性变换的本质是不同视角下的缩放”，所以谱分解存在着一个对称的视角转换。但是，这种观念假设了目标矩阵不隐式表达了旋转操作。换句话说，谱分解的两个对称操作表示了视角的往复，但是假设视角变换的终点并不是回到原地呢？
    - **极分解**：极分解的形式非常简单：$A=RS$，即任何可逆方阵 $A$ 都可以分解为一个旋转（正交矩阵$R$）和一个对称拉伸（对称矩阵$S$）的乘积。也就是说，$A=U\Sigma V^T$，其中 $S=V\Sigma V^T$，$R=UV^T$，即 $U=RV$。极分解解释了实对称矩阵和其它矩阵的差距，而这个差距正是谱分解所疏忽的“纯粹旋转”。也就是说，正交阵自身作为一种“纯粹旋转”，是实对称矩阵的绝对反面。

    任何可逆矩阵的奇异值分解均有唯一的极分解形式，对于不可逆矩阵来说，奇异值分解的双正交阵约束了整个线性变换的趋势，而极分解的旋转矩阵则走向了自由和不唯一；对于非方阵来说，传统意义上的极分解失效了，但如果存在一种广义上的极分解，其中 $S$ 是 $n\times n$ 的对称半正定矩阵，$P$ 是一个 $m\times n$ 的等距同构矩阵，那么 $S=\sqrt{A^TA}=V\sqrt{\Sigma^T\Sigma}V^T$，$P=UV^T$，最终仍会退行回奇异值分解的形式 $A=U\Sigma V^T$。

    综上所述，奇异值分解通过解耦线性变换，实现了某种大一统的线性代数分析法。

## 3. 概率论

### 3.1 概率论基础 (The Foundations)

这是后续所有知识的“公理”和“语法”。

#### 3.1.1 随机事件与概率
- **基本概念**:
    - **样本空间 (Ω)**: 所有可能结果的集合。
    - **随机事件 (A, B)**: 样本空间的一个子集。
- **核心公式**:
    - **加法公式**: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
    - **条件概率**: 在B发生的前提下，A发生的概率。$P(A|B) = \frac{P(A \cap B)}{P(B)}$
    - **乘法公式**: $P(A \cap B) = P(A|B)P(B) = P(B|A)P(A)$
    - **全概率公式**: $P(A) = \sum_{i=1}^{n} P(A|B_i)P(B_i)$ (将复杂事件分解为多种简单情况)
    - **贝叶斯公式 (Bayes' Theorem)**:
      $$P(B_i|A) = \frac{P(A|B_i)P(B_i)}{\sum_{j=1}^{n} P(A|B_j)P(B_j)}$$
      (核心应用：后验概率的计算，是贝叶斯统计和很多机器学习算法的基础)

### 3.2 随机变量及其数字特征

这是将随机事件“数学化”和“量化”的关键。

#### 3.2.1 随机变量 (Random Variable)
- **离散型 (Discrete)**: 取值有限或可数。由**概率质量函数 (PMF)** $P(X=x_k)=p_k$ 描述。
- **连续型 (Continuous)**: 取值在某个区间内不可数。由**概率密度函数 (PDF)** $f(x)$ 描述，满足 $f(x) \ge 0$ 且 $\int_{-\infty}^{\infty}f(x)dx=1$。
- **分布函数 (CDF)**: $F(x) = P(X \le x)$，对离散和连续型都通用。

#### 3.2.2 期望与方差的代数特性
这是**必考**重点，需要熟练运用。

- **期望 (Expectation, E[X])**: 概率意义下的“均值”，描述随机变量的**集中趋势**。
    - **性质**:
        - `E[c] = c` (c为常数)
        - `E[cX] = cE[X]`
        - **`E[X + Y] = E[X] + E[Y]`** (期望的线性性，**不要求**X, Y独立)
        - `E[XY] = E[X]E[Y]` (**要求**X, Y相互独立)

- **方差 (Variance, Var(X))**: 描述随机变量取值相对于其期望的**离散程度**。
    - **定义**: `Var(X) = E[(X - E[X])²] = E[X²] - (E[X])²` (计算方差的常用公式)
    - **性质**:
        - `Var(c) = 0`
        - `Var(X + c) = Var(X)`
        - `Var(cX) = c²Var(X)`
        - **`Var(X + Y) = Var(X) + Var(Y)`** (**要求**X, Y相互独立)
        - `Var(X - Y) = Var(X) + Var(Y)` (**要求**X, Y相互独立)

- **协方差与相关系数**:
    - **协方差 (Covariance)**: `Cov(X, Y) = E[(X-E[X])(Y-E[Y])]`，描述两个变量的线性相关方向。
    - **相关系数 (Correlation)**: $\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$，取值[-1, 1]，描述线性相关强度。

### 3.3 常见概率分布

这是将理论模型应用到现实问题的核心，**判断事件服从什么分布**的能力至关重要。

#### 3.3.1 常用离散分布

1.  **0-1分布 / 伯努利分布 (Bernoulli)**
    - **描述**: 单次随机试验，结果只有两种（成功/失败）。
    - **典型场景**: 抛一次硬币的结果；一件产品是否合格。
    - $P(X=k) = p^k(1-p)^{1-k}$, k=0,1. E[X]=p, Var(X)=p(1-p).

2.  **二项分布 (Binomial, B(n, p))**
    - **描述**: n次**独立重复**的伯努利试验中，成功的次数。
    - **典型场景**: 抛10次硬币，正面朝上恰好3次的概率；射击10次，命中恰好8次的概率。
    - $P(X=k) = C_n^k p^k (1-p)^{n-k}$. E[X]=np, Var(X)=np(1-p).

3.  **泊松分布 (Poisson, P(λ))**
    - **描述**: 单位时间/空间内，某事件发生的次数。
    - **典型场景**: 一小时内到达某路口的车辆数；一页书中印刷错误的个数；一个网站每分钟收到的访问次数。
    - $P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}$. E[X]=λ, Var(X)=λ.
    - **与二项分布关系**: 当n很大，p很小时，二项分布近似于泊松分布（λ=np）。

4.  **几何分布 (Geometric)**
    - **描述**: 在一系列独立重复的伯努利试验中，**首次**成功所需要的试验次数。
    - **典型场景**: 不停地抛硬币，直到第一次出现正面为止，所需要的次数。
    - E[X]=1/p.

#### 3.3.2 常用连续分布

1.  **均匀分布 (Uniform, U(a, b))**
    - **描述**: 在区间 `[a, b]` 内，随机变量取任何值的概率都相等。
    - **典型场景**: 抽奖转盘停在任意角度的概率；一个在[0,1]之间均匀生成的随机数。
    - $f(x) = \frac{1}{b-a}$ for a≤x≤b. E[X]=$\frac{a+b}{2}$, Var(X)=$\frac{(b-a)^2}{12}$.

2.  **指数分布 (Exponential)**
    - **描述**: 泊松过程中，两个事件发生之间的**时间间隔**。
    - **典型场景**: 呼叫中心接到下一个电话需要等待的时间；一个灯泡能正常工作的寿命。
    - **关键特性**: **无记忆性**。`P(X > s+t | X > s) = P(X > t)`。
    - $f(x) = \lambda e^{-\lambda x}$ for x>0. E[X]=1/λ, Var(X)=1/λ².

3.  **正态分布 / 高斯分布 (Normal, N(μ, σ²))**
    - **描述**: 自然界和工程中大量现象的分布规律，由均值`μ`和方差`σ²`决定。
    - **典型场景**: 大量人口的身高、体重；测量误差；大量独立随机变量之和（中心极限定理）。
    - **重要特性**:
        - **标准正态分布 N(0, 1)**: `μ=0, σ=1`。任何正态分布都可以通过 `Z = (X-μ)/σ` 进行标准化。
        - 3σ法则：约99.7%的数据落在`(μ-3σ, μ+3σ)`区间内。

### 3.4 数理统计的桥梁：重要定理

- **大数定律 (Law of Large Numbers)**: 当样本量足够大时，样本均值会收敛于真实期望。这是用采样估计总体的理论基础。
- **中心极限定理 (Central Limit Theorem, CLT)**: 无论原始总体是什么分布，只要样本量`n`足够大，**样本均值的分布**都将近似于**正态分布**。这是正态分布“称霸”统计学的根本原因，也是很多统计检验的理论基石。

### 3.5 推断统计：假设检验

这是利用样本信息来判断关于总体的某个假设是否成立的科学方法。

#### 3.5.1 假设检验的基本逻辑
1.  **建立假设**:
    - **原假设 H₀**: 通常是想要**拒绝**的、保守的、无变化的假设（如“药物无效”、“均值相等”）。
    - **备择假设 H₁**: 我们希望**接受**的、有变化的假设（如“药物有效”、“均值不等”）。
2.  **选择检验统计量**: 根据问题类型选择一种检验方法（如T检验）。
3.  **确定显著性水平α**: 事先设定的犯“弃真”错误的概率上限，通常为0.05或0.01。
4.  **计算p值**: 假设H₀为真时，观测到当前样本结果或更极端结果的概率。
5.  **做出决策**:
    - **如果 p值 < α**: 认为是小概率事件发生了，我们有充分理由**拒绝原假设H₀**。
    - **如果 p值 ≥ α**: 证据不足，我们**不能拒绝原假设H₀**。

#### 3.5.2 常用检验方法

- **正态性检验 (Normality Test)**
    - **目的**: 检验一组样本数据是否来自一个服从正态分布的总体。这是很多其他统计检验（如T检验）的前提。
    - **常用方法**: Shapiro-Wilk检验、Kolmogorov-Smirnov (K-S)检验。
    - **应试理解**: 知道它的**目的**是检验数据是否为正态分布即可。

- **显著性差异检验 (Significance Test)**
    - **目的**: 检验两个或多个组之间，某个指标（通常是均值）的差异是否是“显著的”，而非由随机抽样误差造成的。
    - **最常用方法：T检验 (t-test)**:
        - **适用场景**: 样本量较小（如n<30）且总体方差未知时，检验**一个或两个样本均值**的差异。
        - **类型**:
            - **单样本T检验**: 检验单个样本的均值是否等于一个已知的目标值。
            - **独立双样本T检验**: 检验**两个独立组**的均值是否存在显著差异。（例如：比较A/B测试中两个不同版本广告的点击率均值）
            - **配对样本T检验**: 检验**同一个对象**在两种不同处理前后的均值是否存在显著差异。（例如：检验同一批病人在服药前后的血压均值差异）

## 4. 支持向量机 (SVM)

SVM (Support Vector Machine) 是一种强大的监督学习模型，主要用于分类和回归。其核心思想是找到一个最优的决策边界，使得两类样本之间的间隔（Margin）最大化。

### 4.1 线性可分与硬间隔 SVM

这是最理想的情况，假设两类数据点可以被一条直线（或一个平面）完美地分开。

#### 核心目标：找到最宽的“街道”

- **超平面 (Hyperplane)**: 在二维空间中就是一条直线，三维中是一个平面。它是我们的决策边界。
- **间隔 (Margin)**: 离超平面最近的、分属两类的点（即支持向量）到超平面的距离之和。这就是“街道”的宽度。
- **支持向量 (Support Vectors)**: 那些“贴在街道边缘”上的点，它们是定义决策边界的关键先生。

#### 公式解析

1.  **超平面方程**:
    $$w^T x + b = 0$$
    - `w`: 法向量，决定了超平面的**方向**。
    - `x`: 特征向量。
    - `b`: 偏置项，决定了超平面**在空间中的位置**。

2.  **决策函数**:
    $$f(x) = \text{sign}(w^T x + b)$$
    通过计算结果的符号（+1 或 -1）来判断一个点 `x` 属于哪一类。

3.  **最大化间隔**:
    “街道”的两条边界可以定义为 $w^T x + b = 1$ 和 $w^T x + b = -1$。这两条边界之间的距离，也就是间隔宽度，可以推导为：
    $$\text{Margin} = \frac{2}{\|w\|}$$
    我们的目标是**最大化**这个间隔，这等价于**最小化** `||w||`，为了数学计算方便，我们通常最小化 $\frac{1}{2}\|w\|^2$。

4.  **约束条件**:
    所有点都必须被正确分类，并且不能进入“街道”内部。
    $$y_i(w^T x_i + b) \ge 1 \quad (\text{对于所有样本 } i)$$
    - `yᵢ`: 样本 `i` 的标签（+1 或 -1）。
    - `xᵢ`: 样本 `i` 的特征向量。

**硬间隔SVM的优化问题总结如下：**
$$
\min_{w,b} \frac{1}{2}\|w\|^2 \quad \text{s.t.} \quad y_i(w^T x_i + b) \ge 1, \forall i
$$

### 4.2 应对现实：线性不可分与软间隔 SVM

现实数据往往是嘈杂的，总有几个“离群点”导致数据线性不可分。硬间隔SVM会因此找不到解。

#### 核心目标：允许犯错，但要付出代价

我们引入一个**松弛变量 (Slack Variable)** $\xi_i \ge 0$ (读作 "k'sai")，允许某些点可以“越界”。
- 如果 $\xi_i = 0$：该点严格遵守规则。
- 如果 $0 < \xi_i \le 1$：该点在间隔内，但仍在正确的一侧。
- 如果 $\xi_i > 1$：该点被错误分类。

#### 公式解析

1.  **新的约束条件**:
    $$y_i(w^T x_i + b) \ge 1 - \xi_i \quad (\text{对于所有样本 } i)$$

2.  **新的优化目标**:
    我们既要最小化 $\frac{1}{2}\|w\|^2$ (保持街道尽可能宽)，又要最小化总的“犯错程度” $\sum \xi_i$。
    $$
    \min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{n} \xi_i
    $$

3.  **参数 `C` (正则化系数) 的解析**:
    `C` 是一个超参数，用来平衡“街道宽度”和“犯错代价”之间的**权衡 (Trade-off)**。
    - **`C` 值很大**: 意味着对“犯错”（$\xi_i>0$）的**惩罚极高**。模型会尽力减小 $\xi_i$，哪怕牺牲间隔宽度，也要把大部分点都分类正确。这会导致决策边界变得复杂，容易**过拟合**。
    - **`C` 值很小**: 意味着对“犯错”的**容忍度很高**。模型会更倾向于保持一个很宽的间隔，哪怕因此多错分了几个点。这会导致决策边界比较简单平滑，泛化能力可能更好，但也可能**欠拟合**。

### 4.3 终极武器：非线性问题与核技巧

如果数据分布本身就是弯曲的（例如，一圈点在内，一圈点在外），线性分类器完全无效。

#### 核心目标：升维打击，在高维空间中线性分割

- **核技巧 (The Kernel Trick)**: 它的思想是，我们通过一个非线性函数 $\phi(x)$ 将原始数据映射到一个更高维的空间，在那个新空间里，数据可能就变得线性可分了。
- 核技巧的精髓在于，我们**不需要**真正地计算这个复杂的映射 $\phi(x)$，而是通过一个**核函数 (Kernel Function)** $K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$ 来直接计算高维空间中的点积。

#### 公式解析

1.  **RBF核函数 (径向基函数核)**:
    这是最流行的一种核函数。
    $$K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$$

2.  **参数 `γ` (gamma) 的解析**:
    `γ` 定义了单个训练样本的“影响力”范围。
    - `||xᵢ - xⱼ||²`: 两个点之间距离的平方。
    - **`γ` 值很大**: 指数项会随着距离的增加而**急剧衰减**。这意味着只有非常近的点才会对彼此产生影响。决策边界会变得非常“崎岖”，紧紧地围绕着数据点，模型**非常复杂**，极易**过拟合**。
    - **`γ` 值很小**: 指数项会随着距离的增加而**缓慢衰减**。这意味着一个点可以影响到很远的其他点。决策边界会因此变得非常“平滑”，模型**比较简单**，可能**欠拟合**。

---

### 4.4 快速总结备忘

| 参数 | 值增大 $\uparrow$ | 值减小 $\downarrow$ |
| :--- | :--- | :--- |
| **C** | 惩罚变重，模型更复杂，易**过拟合** | 容忍度高，模型更简单，易**欠拟合** |
| **γ** | 影响半径小，模型更复杂，易**过拟合** | 影响半径大，模型更简单，易**欠拟合** |

## 5.主成分分析 (PCA) 

PCA是一种广泛应用的无监督学习算法，其核心目标是**数据降维 (Dimensionality Reduction)** 和**特征提取 (Feature Extraction)**。

### 5.1 核心目标：信息浓缩与降维

在高维数据中（例如，一张图片有上万个像素特征），特征之间常常存在高度相关性，这意味着信息是冗余的。PCA的目标就是消除这种冗余，找到一组新的、彼此无关的坐标轴（称为“主成分”），并用少数几个新的坐标轴来表示原始数据，同时尽可能多地保留原始数据中的信息。

- **直观理解**: 把一滩“杂乱无章”的数据点，旋转到一个“最佳视角”，使得数据在这个视角下看起来最“分散”（方差最大），从而最能体现其主要结构。

### 5.2 PCA的核心思想：最大方差理论

“信息量”在PCA中被定义为**方差 (Variance)**。一个特征的方差越大，说明它在数据集中变化越大，包含的信息就越多。

- **PCA的目标**: 寻找到一个新的、由一组标准正交基构成的坐标系。其中：
    - **第一主成分 (PC1)**: 是原始数据中**方差最大**的那个方向。
    - **第二主成分 (PC2)**: 是与PC1**正交**的前提下，方差次大的方向。
    - **第三主成分 (PC3)**: 是与PC1和PC2都**正交**的前提下，方差再次大的方向。
    - ...以此类推。

通过保留前 `k` 个方差最大的主成分，我们就得到了一个能最大程度保留原始信息（方差）的 `k` 维数据表示。

### 5.3 数学基石：协方差矩阵的特征值分解

这个“寻找最大化方差的正交方向”的优化目标，在数学上完美地等价于**对数据的协方差矩阵进行特征值分解**。

- **协方差矩阵 (Covariance Matrix) `C`**:
    - 一个实对称矩阵。
    - 其对角线元素是各个原始特征的方差。
    - 其非对角线元素是不同原始特征之间的协方差。

- **核心联系**:
    - **协方差矩阵的特征向量 (Eigenvectors)**: 正是我们要寻找的**主成分方向**。因为协方差矩阵是实对称的，这些特征向量**天然就是正交的**！
    - **协方差矩阵的特征值 (Eigenvalues)**: 恰好是数据在对应主成分方向（特征向量）上的**方差大小**。特征值越大，说明该主成分越重要。

这个美妙的对应关系，让我们之前讨论的“实对称矩阵”和“特征值分解”的知识有了用武之地。

### 5.4 PCA的计算步骤

假设我们有一个 `n x p` 的数据矩阵 `X`（n个样本，p个特征）。

1.  **数据中心化 (Data Centering)**
    对每一个特征（每一列），减去该特征的均值。
    $$X' = X - \text{mean}(X)$$
    **目的**: 确保我们分析的是数据围绕其中心的散布情况，而不是围绕坐标原点。

2.  **计算协方差矩阵 `C`**
    $$C = \frac{1}{n-1} X'^T X'$$
    这是一个 `p x p` 的实对称矩阵。

3.  **对协方差矩阵 `C` 进行特征值分解**
    计算出所有的特征值 $\lambda_1, \lambda_2, \dots, \lambda_p$ 和对应的特征向量 $\vec{v}_1, \vec{v}_2, \dots, \vec{v}_p$。

4.  **排序并选择主成分**
    将特征值按从大到小的顺序排列。对应的特征向量也随之排序。最大的特征值 $\lambda_1$ 对应的特征向量 $\vec{v}_1$ 就是第一主成分 (PC1)，以此类推。

5.  **转换到新空间（降维）**
    选择前 `k` 个最重要的主成分（即前k个特征向量），将它们作为列向量，组成一个 `p x k` 的变换矩阵 `W`。
    $$W = [\vec{v}_1, \vec{v}_2, \dots, \vec{v}_k]$$
    用原始的中心化数据 `X'` 乘以这个变换矩阵，就得到了降维后的 `n x k` 新数据矩阵 `Z`。
    $$Z = X'W$$

### 5.5 PCA与SVD的联系 (更高效的实现方式)

在实践中，尤其是当特征维度 `p` 非常高时，计算 `p x p` 的协方差矩阵并对其进行特征值分解的计算成本很高。这时，SVD提供了一个更直接、数值更稳定的方法。

对**中心化后**的数据矩阵 `X'` 直接进行SVD分解：
$$X'_{n \times p} = U_{n \times n} \Sigma_{n \times p} V_{p \times p}^T$$

我们之前推导过，`X'^T X' = V\Sigma^T\Sigma V^T`。这正是协方差矩阵的特征值分解形式！

- **核心联系**:
    - SVD的**右奇异向量** (矩阵`V`的列) 就是我们要找的**主成分**（等价于协方差矩阵的特征向量）。
    - **奇异值的平方** (`σᵢ²`) 与协方差矩阵的**特征值**成正比（$\lambda_i = \frac{\sigma_i^2}{n-1}$）。

因此，通过对数据矩阵`X'`进行SVD，我们可以直接得到主成分`V`和相关的方差信息，从而绕过了计算庞大的协方差矩阵这一步。

### 5.6 实践中的考量：如何选择`k`？

- **解释方差比 (Explained Variance Ratio)**
    计算每个主成分所解释的方差占总方差的比例 $\frac{\lambda_i}{\sum \lambda_j}$，然后看前 `k` 个主成分的累计方差贡献率是否达到一个阈值（例如95%或99%）。
- **碎石图 (Scree Plot)**
    将排序后的特征值（方差）绘制成条形图。通常图形会有一个明显的“肘部”，即特征值从大到小急剧下降后变得平缓的拐点。这个拐点之后的主成分通常可以被忽略。
